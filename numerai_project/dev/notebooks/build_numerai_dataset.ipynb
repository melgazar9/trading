{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook builds the stock market / numerai dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make cells wider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "breeding-triumph"
   },
   "source": [
    "### Imports\n",
    "All other necessary imports are imported via python scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "amber-cabin"
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "###### Imports ######\n",
    "#####################\n",
    "\n",
    "import os\n",
    "from configparser import ConfigParser\n",
    "import sys\n",
    "from IPython.display import display\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numerapi\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "if not os.getcwd().endswith('trading'): os.chdir('../../..') # local machine\n",
    "\n",
    "assert os.getcwd().endswith('trading'), 'Wrong path!'\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = '32'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '16'\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "from dev.scripts.ML_utils import * # run if on local machine\n",
    "from dev.scripts.trading_utils import * # run if on local machine\n",
    "from numerai.dev.scripts.numerai_utils import *\n",
    "from numerai.dev.configs.build_numerai_dataset_cfg import *\n",
    "\n",
    "\n",
    "###  pd options / configs ###\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "config = ConfigParser()\n",
    "config.read('numerai/numerai_keys.ini')\n",
    "\n",
    "### connect to the numerai signals API ###\n",
    "\n",
    "napi = numerapi.SignalsAPI(config['KEYS']['NUMERAI_PUBLIC_KEY'], config['KEYS']['NUMERAI_SECRET_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of eligible tickers: 5435\n",
      "Number of eligible tickers in map: 5435\n",
      "tickers before cleaning: (5435, 3)\n",
      "tickers after cleaning: (5385, 3)\n"
     ]
    }
   ],
   "source": [
    "### download data ###\n",
    "\n",
    "if DOWNLOAD_NUMERAI_COMPETITION_DATA:\n",
    "    # napi = numerapi.NumerAPI(NUMERAI_PUBLIC_KEY, NUMERAI_SECRET_KEY)\n",
    "    napi.download_current_dataset(unzip=True)\n",
    "if LOAD_NUMERAI_COMPETITION_DATA:\n",
    "    df_numerai_comp = dd.read_csv(DF_NUMERAI_COMP_TRAIN_PATH).compute()\n",
    "\n",
    "\n",
    "### Load eligible tickers ###\n",
    "\n",
    "eligible_tickers = pd.Series(napi.ticker_universe(), name='ticker')\n",
    "\n",
    "ticker_map = pd.read_csv('https://numerai-signals-public-data.s3-us-west-2.amazonaws.com/signals_ticker_map_w_bbg.csv')\n",
    "ticker_map = ticker_map[ticker_map[TICKER_COL].isin(eligible_tickers)]\n",
    "\n",
    "if VERBOSE:\n",
    "    print(f\"Number of eligible tickers: {len(eligible_tickers)}\")\n",
    "    print(f\"Number of eligible tickers in map: {len(ticker_map)}\")\n",
    "\n",
    "# Remove null / empty tickers from the yahoo tickers\n",
    "valid_tickers = [i for i in ticker_map['yahoo']\n",
    "     if not pd.isnull(i)\n",
    "     and not str(i).lower()=='nan' \\\n",
    "     and not str(i).lower()=='null' \\\n",
    "     and not str(i).lower()==''\\\n",
    "     and len(i) > 0\\\n",
    "]\n",
    "\n",
    "if VERBOSE: print('tickers before cleaning:', ticker_map.shape) # before removing bad tickers\n",
    "ticker_map = ticker_map[ticker_map['yahoo'].isin(valid_tickers)]\n",
    "if VERBOSE: print('tickers after cleaning:', ticker_map.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "DOWNLOAD_YAHOO_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download or load in yahoo finance data in the expected numerai format using the yfinance library ###\n",
    "# Yahoo Finance wrappers: https://github.com/ranaroussi/yfinance and https://pypi.org/project/yfinance/.\n",
    "# Downloading ~2 hours on a single-thread\n",
    "\n",
    "if DOWNLOAD_YAHOO_DATA:\n",
    "    df_yahoo = download_yfinance_data(list(ticker_map['yahoo']), **DOWNLOAD_YFINANCE_DATA_PARAMS) # all valid yahoo tickers\n",
    "else: # read in file\n",
    "    if YAHOO_READ_FILEPATH.lower().endswith('pq') or YAHOO_READ_FILEPATH.lower().endswith('parquet'):\n",
    "        df_yahoo = dd.read_parquet(YAHOO_READ_FILEPATH, DASK_NPARTITIONS=DASK_NPARTITIONS).compute()\n",
    "    elif YAHOO_READ_FILEPATH.lower().endswith('feather'):\n",
    "        df_yahoo = pd.read_feather(YAHOO_READ_FILEPATH)\n",
    "# df_yahoo = df_yahoo.tail(1000000)# debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21198389 entries, 0 to 21198388\n",
      "Columns: 153 entries, date to bloomberg_ticker\n",
      "dtypes: category(2), datetime64[ns](1), float32(150)\n",
      "memory usage: 12.1 GB\n",
      "None\n",
      "\n",
      "converting dtypes...\n",
      "\n",
      "\n",
      "validating unique date + ticker index...\n",
      "\n",
      "\n",
      "sorting...\n",
      "\n",
      "\n",
      "saving...\n",
      "\n",
      "\n",
      "reading targets...\n",
      "\n",
      "\n",
      "merging numerai target...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if VERBOSE: print(df_yahoo.info())\n",
    "gc.collect()\n",
    "if CONVERT_DF_DTYPES:\n",
    "    print('\\nconverting dtypes...\\n')\n",
    "    df_yahoo = convert_df_dtypes(df_yahoo, **CONVERT_DTYPE_PARAMS)\n",
    "\n",
    "if CREATE_BLOOMBERG_TICKER_FROM_YAHOO or DOWNLOAD_YAHOO_DATA:\n",
    "    if ('yahoo_ticker' not in df_yahoo.columns) or ('ticker' in df_yahoo.columns):\n",
    "        df_yahoo.rename(columns={'ticker': 'yahoo_ticker'}, inplace=True)\n",
    "    df_yahoo.loc[:, 'bloomberg_ticker'] = df_yahoo['yahoo_ticker'].map(dict(zip(ticker_map['yahoo'], ticker_map['bloomberg_ticker'])))\n",
    "\n",
    "\n",
    "### Ensure no [DATETIME_COL, TICKER_COL] are duplicated. If so then there is an issue. ###\n",
    "\n",
    "print('\\nvalidating unique date + ticker index...\\n')\n",
    "\n",
    "datetime_ticker_cat = (df_yahoo[DATETIME_COL].astype(str) + ' ' + df_yahoo[TICKER_COL].astype(str)).tolist()\n",
    "assert len(datetime_ticker_cat) == len(set(datetime_ticker_cat)), 'TICKER_COL and DATETIME_COL do not make a unique index!'\n",
    "del datetime_ticker_cat\n",
    "\n",
    "if DROP_DUPLICATE_ROWS: df_yahoo.drop_duplicates(inplace=True)\n",
    "\n",
    "print('\\nsorting...\\n')\n",
    "\n",
    "df_yahoo.sort_values(by=[DATETIME_COL, TICKER_COL], inplace=True)\n",
    "\n",
    "print('\\nsaving...\\n')\n",
    "\n",
    "if INIT_SAVE_FILEPATH.endswith('feather'):\n",
    "    if 'date' in df_yahoo.index.names or 'ticker' in df_yahoo.index.names:\n",
    "        df_yahoo.reset_index().to_feather(INIT_SAVE_FILEPATH)\n",
    "    else:\n",
    "        df_yahoo.reset_index(drop=True).to_feather(INIT_SAVE_FILEPATH)\n",
    "elif INIT_SAVE_FILEPATH.endswith('pq') or INIT_SAVE_FILEPATH.endswith('parquet'):\n",
    "    df_yahoo.to_parquet(INIT_SAVE_FILEPATH)\n",
    "\n",
    "print('\\nreading targets...\\n')\n",
    "\n",
    "targets = pd.read_csv(NUMERAI_TARGETS_URL).assign(date=lambda df: pd.to_datetime(df['friday_date'], format='%Y%m%d'))\n",
    "\n",
    "if VERBOSE: targets['target'].value_counts(), targets['target'].value_counts(normalize=True)\n",
    "\n",
    "### Merge targets into df_yahoo ###\n",
    "\n",
    "# - From an inner join on `['date', 'bloomberg_ticker']` we lose about 85% of rows.\n",
    "# - If we drop rows with NAs we have 0 rows left no matter what.\n",
    "# - The best bet seems to be an outer join without dropping NA rows.\n",
    "# df_yahoo.set_index(DATETIME_COL, inplace=True)\n",
    "# df_yahoo.sort_index(inplace=True)\n",
    "print('\\nmerging numerai target...\\n')\n",
    "df_yahoo = pd.merge(df_yahoo, targets, on=TARGET_JOIN_COLS, how=TARGET_JOIN_METHOD)\n",
    "del targets # reduce memory\n",
    "TICKERS = df_yahoo[TICKER_COL].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>yahoo_ticker</th>\n",
       "      <th>adj_close_1d</th>\n",
       "      <th>close_1d</th>\n",
       "      <th>high_1d</th>\n",
       "      <th>...</th>\n",
       "      <th>data_type</th>\n",
       "      <th>target</th>\n",
       "      <th>day_name</th>\n",
       "      <th>friday_date_name</th>\n",
       "      <th>is_friday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990-01-01</td>\n",
       "      <td>ABF.L</td>\n",
       "      <td>112.04471</td>\n",
       "      <td>236.93201</td>\n",
       "      <td>236.93201</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Monday</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1990-01-01</td>\n",
       "      <td>AHT.L</td>\n",
       "      <td>22.82121</td>\n",
       "      <td>50.40700</td>\n",
       "      <td>50.40700</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Monday</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1990-01-01</td>\n",
       "      <td>ALPHA.AT</td>\n",
       "      <td>72.28684</td>\n",
       "      <td>81.67100</td>\n",
       "      <td>81.67100</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Monday</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990-01-01</td>\n",
       "      <td>ANN.AX</td>\n",
       "      <td>14.14669</td>\n",
       "      <td>22.51082</td>\n",
       "      <td>22.51082</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Monday</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1990-01-01</td>\n",
       "      <td>ANTO.L</td>\n",
       "      <td>8.75124</td>\n",
       "      <td>19.36540</td>\n",
       "      <td>19.36540</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Monday</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 159 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date yahoo_ticker  adj_close_1d  close_1d   high_1d  ...  data_type  \\\n",
       "0 1990-01-01        ABF.L     112.04471 236.93201 236.93201  ...        NaN   \n",
       "1 1990-01-01        AHT.L      22.82121  50.40700  50.40700  ...        NaN   \n",
       "2 1990-01-01     ALPHA.AT      72.28684  81.67100  81.67100  ...        NaN   \n",
       "3 1990-01-01       ANN.AX      14.14669  22.51082  22.51082  ...        NaN   \n",
       "4 1990-01-01       ANTO.L       8.75124  19.36540  19.36540  ...        NaN   \n",
       "\n",
       "   target  day_name  friday_date_name  is_friday  \n",
       "0     NaN    Monday               NaN          0  \n",
       "1     NaN    Monday               NaN          0  \n",
       "2     NaN    Monday               NaN          0  \n",
       "3     NaN    Monday               NaN          0  \n",
       "4     NaN    Monday               NaN          0  \n",
       "\n",
       "[5 rows x 159 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yahoo['day_name'] = pd.to_datetime(df_yahoo['date'], format='%Y%m%d').dt.day_name()\n",
    "df_yahoo['friday_date_name'] = pd.to_datetime(df_yahoo['friday_date'], format='%Y%m%d').dt.day_name()\n",
    "df_yahoo['is_friday'] = 0\n",
    "df_yahoo.loc[df_yahoo['day_name'].str.lower() == 'friday', 'is_friday'] = 1\n",
    "df_yahoo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4305154, 6114763, 4305154)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yahoo['friday_date'].notnull().sum(), df_yahoo['is_friday'].sum(), df_yahoo[df_yahoo['target'].notnull()]['is_friday'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./numerai_signals_historical.csv: 100%|█████████▉| 127M/127M [00:16<00:00, 9.58MB/s] "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bloomberg_ticker</th>\n",
       "      <th>friday_date</th>\n",
       "      <th>data_type</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000270 KS</td>\n",
       "      <td>20030131</td>\n",
       "      <td>train</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000810 KS</td>\n",
       "      <td>20030131</td>\n",
       "      <td>train</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000830 KS</td>\n",
       "      <td>20030131</td>\n",
       "      <td>train</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002790 KS</td>\n",
       "      <td>20030131</td>\n",
       "      <td>train</td>\n",
       "      <td>0.25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003450 KS</td>\n",
       "      <td>20030131</td>\n",
       "      <td>train</td>\n",
       "      <td>0.25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  bloomberg_ticker  friday_date data_type  target\n",
       "0        000270 KS     20030131     train 0.50000\n",
       "1        000810 KS     20030131     train 0.50000\n",
       "2        000830 KS     20030131     train 0.50000\n",
       "3        002790 KS     20030131     train 0.25000\n",
       "4        003450 KS     20030131     train 0.25000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run submit test\n",
    "df_val = pd.read_csv(napi.download_validation_data())\n",
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Friday    4305154\n",
       "Name: is_friday, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "./numerai_signals_historical.csv: 127MB [00:29, 9.58MB/s]                           "
     ]
    }
   ],
   "source": [
    "df_val['is_friday'] = pd.to_datetime(df_val['friday_date'], format='%Y%m%d').dt.day_name()\n",
    "df_val['is_friday'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = df_val[df_val['data_type']=='validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2156196, 162), (2156196, 5), (2156196, 159))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_submit = pd.merge(df_yahoo, df_val, on=['bloomberg_ticker', 'friday_date'], how='inner')\n",
    "df_to_submit.shape, df_val.shape, df_yahoo[(df_yahoo['target'].notnull()) & (df_yahoo['is_friday'] == 1) & (df_yahoo['data_type'] == 'validation')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### conditionally drop NAs ###\n",
    "\n",
    "if RUN_CONDITIONAL_DROPNA:\n",
    "    print('\\nrunning conditional drop_nas...\\n')\n",
    "    df_yahoo = drop_nas(df_yahoo, **DROPNA_PARAMS)\n",
    "gc.collect()\n",
    "\n",
    "### create naive features ###\n",
    "\n",
    "print('\\ncreating naive features...\\n')\n",
    "\n",
    "df_yahoo = df_yahoo.groupby(TICKER_COL, group_keys=False).apply(lambda df: create_naive_features_single_symbol(df, **NAIVE_FEATURES_PARAMS)) # Create naive features (e.g. moves, ranges, etc...)\n",
    "\n",
    "### create diff features ###\n",
    "\n",
    "diff_params = eval(DIFF_PARAMS_STRING)\n",
    "df_yahoo = df_yahoo.groupby(TICKER_COL, group_keys=False).apply(lambda df: calc_diffs(df, **diff_params))\n",
    "\n",
    "### create pct_change features ###\n",
    "\n",
    "pct_change_params = eval(PCT_CHG_PARAMS_STRING)\n",
    "df_yahoo = df_yahoo.groupby(TICKER_COL, group_keys=False).apply(lambda df: calc_pct_changes(df, **pct_change_params))\n",
    "\n",
    "### create custom targets ###\n",
    "\n",
    "print('\\ncreating custom targets...\\n')\n",
    "\n",
    "df_yahoo = CreateTargets(df_yahoo, copy=False).create_targets_HL3(**TARGETS_HL3_PARAMS) # create target_HL3\n",
    "df_yahoo = CreateTargets(df_yahoo, copy=False).create_targets_HL5(**TARGETS_HL5_PARAMS) # create target_HL5\n",
    "\n",
    "if VERBOSE:\n",
    "    display(df_yahoo[TARGETS_HL3_PARAMS['target_suffix']].value_counts()), display(df_yahoo[TARGETS_HL3_PARAMS['target_suffix']].value_counts(normalize=True))\n",
    "    display(df_yahoo[TARGETS_HL5_PARAMS['target_suffix']].value_counts()), display(df_yahoo[TARGETS_HL5_PARAMS['target_suffix']].value_counts(normalize=True))\n",
    "\n",
    "\n",
    "### For each ticker, for non-numerai data, shift the target backwards one timestamp, where each row is the unit of measure (e.g. each row is a day) ###\n",
    "\n",
    "if SHIFT_TARGET_HL_UP_TO_PRED_FUTURE:\n",
    "    df_yahoo[TARGETS_HL3_PARAMS['target_suffix']] = df_yahoo.groupby(TICKER_COL)[TARGETS_HL3_PARAMS['target_suffix']].transform(lambda col: col.shift(-1))\n",
    "    df_yahoo[TARGETS_HL5_PARAMS['target_suffix']] = df_yahoo.groupby(TICKER_COL)[TARGETS_HL5_PARAMS['target_suffix']].transform(lambda col: col.shift(-1))\n",
    "\n",
    "### save memory 2 ###\n",
    "\n",
    "if CONVERT_DF_DTYPES:\n",
    "    print('\\nconverting dtypes...\\n')\n",
    "    df_yahoo = convert_df_dtypes(df_yahoo, **CONVERT_DTYPE_PARAMS)\n",
    "\n",
    "### Create lagging features ###\n",
    "print('\\ncreating lagging features...\\n')\n",
    "df_yahoo = df_yahoo.groupby(TICKER_COL, group_keys=False).apply(lambda df: create_lagging_features(df, **LAGGING_FEATURES_PARAMS))\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "### Create rolling features ###\n",
    "\n",
    "# df_yahoo = create_rolling_features(df_yahoo, **ROLLING_FEATURES_PARAMS)\n",
    "print('\\ncreating rolling features...\\n')\n",
    "df_yahoo = df_yahoo.groupby(TICKER_COL, group_keys=False).apply(lambda df: create_rolling_features(df, **ROLLING_FEATURES_PARAMS))\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "### Create move_iar features ###\n",
    "\n",
    "print('\\ncalculating move_iar...\\n')\n",
    "df_yahoo = df_yahoo.groupby(TICKER_COL, group_keys=False).apply(lambda df: calc_move_iar(df, **IAR_PARAMS))\n",
    "gc.collect()\n",
    "\n",
    "### save memory 3 ###\n",
    "\n",
    "if CONVERT_DF_DTYPES:\n",
    "    print('\\nconverting dtypes...\\n')\n",
    "    df_yahoo = convert_df_dtypes(df_yahoo, **CONVERT_DTYPE_PARAMS)\n",
    "\n",
    "### Save df ###\n",
    "\n",
    "print('\\nfinal save...\\n')\n",
    "if FINAL_SAVE_FILEPATH.endswith('feather'):\n",
    "    if 'date' in df_yahoo.index.names or 'ticker' in df_yahoo.index.names:\n",
    "        df_yahoo.reset_index().to_feather(FINAL_SAVE_FILEPATH)\n",
    "    else:\n",
    "        df_yahoo.reset_index(drop=True).to_feather(FINAL_SAVE_FILEPATH)\n",
    "elif FINAL_SAVE_FILEPATH.endswith('pq') or FINAL_SAVE_FILEPATH.endswith('parquet'):\n",
    "    df_yahoo.to_parquet(FINAL_SAVE_FILEPATH)\n",
    "\n",
    "end_time = time.time()\n",
    "if VERBOSE: print('Script took:', round((end_time - start_time) / 60, 3), 'minutes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RU-UGOk2rA8w"
   },
   "source": [
    "### Only run the below if on google colab notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5vQqHDb0qRLd",
    "outputId": "6731e0c2-99ac-40b0-b86c-5ecf33dbb34f"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UreNrl9srNXa"
   },
   "outputs": [],
   "source": [
    "# sys.path.append('/content/gdrive/trading/dev/scripts/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0l6obyRwr9qy"
   },
   "outputs": [],
   "source": [
    "# from gdrive.MyDrive.trading.dev.scripts.ML_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  pd options / configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['numerai/numerai_keys.ini']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "config = ConfigParser()\n",
    "\n",
    "config.read('numerai/numerai_keys.ini')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### connect to the numerai signals API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "napi = numerapi.SignalsAPI(config['KEYS']['NUMERAI_PUBLIC_KEY'], config['KEYS']['NUMERAI_SECRET_KEY'])\n",
    "\n",
    "### download data ###\n",
    "\n",
    "if DOWNLOAD_NUMERAI_COMPETITION_DATA:\n",
    "    # napi = numerapi.NumerAPI(NUMERAI_PUBLIC_KEY, NUMERAI_SECRET_KEY)\n",
    "    napi.download_current_dataset(unzip=True)\n",
    "if LOAD_NUMERAI_COMPETITION_DATA:\n",
    "    df_numerai_comp = dd.read_csv(DF_NUMERAI_COMP_TRAIN_PATH).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load eligible tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of eligible tickers: 5430\n",
      "Number of eligible tickers in map: 5430\n",
      "tickers before: (5430, 3)\n",
      "tickers after: (5380, 3)\n"
     ]
    }
   ],
   "source": [
    "eligible_tickers = pd.Series(napi.ticker_universe(), name='ticker')\n",
    "\n",
    "ticker_map = pd.read_csv('https://numerai-signals-public-data.s3-us-west-2.amazonaws.com/signals_ticker_map_w_bbg.csv')\n",
    "ticker_map = ticker_map[ticker_map[TICKER_COL].isin(eligible_tickers)]\n",
    "\n",
    "if VERBOSE:\n",
    "    print(f\"Number of eligible tickers: {len(eligible_tickers)}\")\n",
    "    print(f\"Number of eligible tickers in map: {len(ticker_map)}\")\n",
    "\n",
    "# Remove null / empty tickers from the yahoo tickers\n",
    "valid_tickers = [i for i in ticker_map['yahoo']\n",
    "     if not pd.isnull(i)\n",
    "     and not str(i).lower()=='nan' \\\n",
    "     and not str(i).lower()=='null' \\\n",
    "     and not str(i).lower()==''\\\n",
    "]\n",
    "\n",
    "if VERBOSE: print('tickers before:', ticker_map.shape) # before removing bad tickers\n",
    "ticker_map = ticker_map[ticker_map['yahoo'].isin(valid_tickers)]\n",
    "if VERBOSE: print('tickers after:', ticker_map.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download or load in yahoo finance data in the expected numerai format using the yfinance library\n",
    "Yahoo Finance wrappers: https://github.com/ranaroussi/yfinance and https://pypi.org/project/yfinance/. <br>\n",
    "Downloading ~2 hours on a single-thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals_to_download = ['1d', '1h']\n",
    "join_method = 'outer'\n",
    "max_intraday_lookback_days = 363\n",
    "n_chunks = 600\n",
    "yfinance_params = {'start': '2021-01-01', 'threads': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "delayed_list = []\n",
    "i = intervals_to_download[0]\n",
    "yfinance_params['interval'] = i\n",
    "yfinance_params2 = yfinance_params.copy()\n",
    "yfinance_params2['interval'] = i\n",
    "intraday_lookback_days = datetime.datetime.today().date() - datetime.timedelta(days=max_intraday_lookback_days)\n",
    "if pd.to_datetime(yfinance_params2['start']) < intraday_lookback_days:\n",
    "    yfinance_params2['start'] = str(intraday_lookback_days)\n",
    "tickers = list(ticker_map['yahoo'])\n",
    "num_workers=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers2 = tickers[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " *** Pulling yfinance data using 32 threads! ***\n",
      "Running safer-parellel\n"
     ]
    }
   ],
   "source": [
    "dl = download_yfinance_data(tickers2, n_chunks=30,\n",
    "                            num_workers=32,\n",
    "                            yfinance_params={'threads':False, 'start': '2021-01-01'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers2 = tickers[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_len = len(tickers2) // num_workers\n",
    "ticker_chunks = [' '.join(tickers2[i:i+chunk_len]) for i in range(0, len(tickers2), chunk_len)]\n",
    "chunk_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_yfinance_data(tickers,\n",
    "                           intervals_to_download=['1d', '1h'],\n",
    "                           num_workers=1,\n",
    "                           join_method='outer',\n",
    "                           max_intraday_lookback_days=363,\n",
    "                           n_chunks=600,\n",
    "                           yfinance_params={}):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    __________\n",
    "\n",
    "    See yfinance.download docs for a detailed description of yfinance parameters\n",
    "\n",
    "    tickers : list of tickers to pass to yfinance.download - it will be parsed to be in the format \"AAPL MSFT FB\"\n",
    "    intervals_to_download : list of intervals to download OHLCV data for each stock (e.g. ['1w', '1d', '1h'])\n",
    "    num_workers : number of threads used to download the data\n",
    "        so far only 1 thread is implemented\n",
    "    join_method : can be 'inner', 'left', 'right' or 'outer'\n",
    "        if 'outer' then all dates will be present\n",
    "        if 'left' then all dates from the left table will be present\n",
    "        if 'right' then all dates from the right table will be present\n",
    "        if 'inner' then all dates must match for each ticker\n",
    "    **yfinance_params : dict - passed to yfinance.dowload(yfinance_params)\n",
    "        set threads = True for faster performance, but tickers will fail, scipt may hang\n",
    "        set threads = False for slower performance, but more tickers will succeed\n",
    "\n",
    "    NOTE: passing some intervals return unreliable stock data (e.g. '3mo' returns many NA data points when they should not be NA)\n",
    "    \"\"\"\n",
    "\n",
    "    intraday_lookback_days = datetime.datetime.today().date() - datetime.timedelta(days=max_intraday_lookback_days)\n",
    "    start_date = yfinance_params['start']\n",
    "\n",
    "    if num_workers == 1:\n",
    "\n",
    "        list_of_dfs = []\n",
    "\n",
    "        for i in intervals_to_download:\n",
    "\n",
    "            yfinance_params['interval'] = i\n",
    "\n",
    "            if (i.endswith('m') or i.endswith('h')) and (pd.to_datetime(yfinance_params['start']) < intraday_lookback_days):\n",
    "                yfinance_params['start'] = str(intraday_lookback_days)\n",
    "\n",
    "            if yfinance_params['threads'] == True:\n",
    "\n",
    "                df_i = yfinance.download(' '.join(tickers), **yfinance_params)\\\n",
    "                               .stack()\\\n",
    "                               .rename_axis(index=['date', 'ticker'])\\\n",
    "                               .add_suffix('_' + i)\\\n",
    "                               .reset_index()\n",
    "            else:\n",
    "\n",
    "                ticker_chunks = [' '.join(tickers[i:i+n_chunks]) for i in range(0, len(tickers), n_chunks)]\n",
    "                chunk_dfs_lst = []\n",
    "\n",
    "                for chunk in ticker_chunks:\n",
    "                    try:\n",
    "                        df_tmp = yfinance.download(chunk, **yfinance_params)\\\n",
    "                                         .stack()\\\n",
    "                                         .rename_axis(index=['date', 'ticker'])\\\n",
    "                                         .add_suffix('_' + i)\\\n",
    "                                         .reset_index()\n",
    "                        chunk_dfs_lst.append(df_tmp)\n",
    "                    except simplejson.errors.JSONDecodeError:\n",
    "                        pass\n",
    "\n",
    "                df_i = pd.concat(chunk_dfs_lst)\n",
    "                del chunk_dfs_lst\n",
    "                yfinance_params['start'] = start_date\n",
    "\n",
    "            if i.endswith('m') or i.endswith('h'):\n",
    "                # Go long-to-wide on the min/hour bars\n",
    "                df_i = df_i.pivot_table(index=[df_i['date'].dt.date, 'ticker'], columns=[df_i['date'].dt.hour], aggfunc='first',\n",
    "                                        values=[i for i in df_i.columns if not i in ['date', 'ticker']])\n",
    "                df_i.columns = list(pd.Index([str(e[0]).lower() + '_' + str(e[1]).lower() for e in df_i.columns.tolist()]).str.replace(' ', '_'))\n",
    "                df_i.reset_index(inplace=True)\n",
    "                df_i['date'] = pd.to_datetime(df_i['date']) # pivot table sets the index, and reset_index changes 'date' to an object\n",
    "\n",
    "            df_i.columns = [col.replace(' ', '_').lower() for col in df_i.columns]\n",
    "\n",
    "            list_of_dfs.append(df_i)\n",
    "\n",
    "        df_yahoo = reduce(lambda x, y: pd.merge(x, y, how=join_method, on=['date', 'ticker']), list_of_dfs)\n",
    "        date_plus_ticker = df_yahoo['date'].astype(str) + df_yahoo['ticker'].astype(str) # one last quality check to ensure date + ticker is unique\n",
    "\n",
    "        assert len(date_plus_ticker) == len(set(date_plus_ticker)), i + ' date + ticker is not unique in df_yahoo!'\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(' *** Pulling yfinance data using', num_workers, 'threads! ***')\n",
    "        list_of_dfs = []\n",
    "        chunk_len = len(tickers) // num_workers\n",
    "        ticker_chunks = [' '.join(tickers[i:i+chunk_len]) for i in range(0, len(tickers), chunk_len)]\n",
    "\n",
    "        for i in intervals_to_download:\n",
    "\n",
    "            yfinance_params['interval'] = i\n",
    "\n",
    "            if (i.endswith('m') or i.endswith('h')) and (pd.to_datetime(yfinance_params['start']) < intraday_lookback_days):\n",
    "                yfinance_params['start'] = str(intraday_lookback_days)\n",
    "\n",
    "            if yfinance_params['threads'] == True:\n",
    "\n",
    "                print('Parallelizing using both dask and yfinance threads - some tickers may return a JSONDecodeError. If so, set threads to False in yfinance_params')\n",
    "                \n",
    "                delayed_list = [delayed(yfinance.download)(' '.join(chunk), **yfinance_params)\\\n",
    "                                                              .stack()\\\n",
    "                                                              .rename_axis(index=['date', 'ticker'])\\\n",
    "                                                              .add_suffix('_' + i)\\\n",
    "                                                              .reset_index()\\\n",
    "                                for chunk in ticker_chunks]\n",
    "                # tuple_of_dfs = dask.compute(*delayed_list, num_workers=num_workers)\n",
    "                # df_i = reduce(lambda x, y: pd.merge(x, y, how=join_method, on=['date', 'ticker']), tuple_of_dfs)\n",
    "                # del tuple_of_dfs\n",
    "\n",
    "            else:\n",
    "\n",
    "                print('Running safer-parallel')\n",
    "\n",
    "                def safe_yfinance_pull(ticker_chunks, yfinance_params):\n",
    "\n",
    "                    chunk_dfs_lst = []\n",
    "\n",
    "                    for chunk in ticker_chunks:\n",
    "                        try:\n",
    "                            df_tmp = yfinance.download(chunk, **yfinance_params)\\\n",
    "                                             .stack()\\\n",
    "                                             .rename_axis(index=['date', 'ticker'])\\\n",
    "                                             .add_suffix('_' + i)\\\n",
    "                                             .reset_index()\n",
    "                            chunk_dfs_lst.append(df_tmp)\n",
    "                        except simplejson.errors.JSONDecodeError:\n",
    "                            pass\n",
    "\n",
    "                    df_out = pd.concat(chunk_dfs_lst)\n",
    "                    return df_out\n",
    "\n",
    "                delayed_list = [delayed(safe_yfinance_pull)(chunk, yfinance_params) for chunk in ticker_chunks]\n",
    "\n",
    "            tuple_of_dfs = dask.compute(*delayed_list, num_workers=num_workers)\n",
    "\n",
    "            df_i = reduce(lambda x, y: pd.merge(x, y, how=join_method, on=['date', 'ticker']), tuple_of_dfs)\n",
    "            del tuple_of_dfs\n",
    "            yfinance_params['start'] = start_date\n",
    "\n",
    "            if i.endswith('m') or i.endswith('h'):\n",
    "                # Go long-to-wide on the min/hour bars\n",
    "                df_i = df_i.pivot_table(index=[df_i['date'].dt.date, 'ticker'], columns=[df_i['date'].dt.hour], aggfunc='first',\n",
    "                                        values=[i for i in df_i.columns if not i in ['date', 'ticker']])\n",
    "                df_i.columns = list(pd.Index([str(e[0]).lower() + '_' + str(e[1]).lower() for e in df_i.columns.tolist()]).str.replace(' ', '_'))\n",
    "                df_i.reset_index(inplace=True)\n",
    "                df_i['date'] = pd.to_datetime(df_i['date']) # pivot table sets the index, and reset_index changes 'date' to an object\n",
    "\n",
    "            df_i.columns = [col.replace(' ', '_').lower() for col in df_i.columns]\n",
    "\n",
    "            list_of_dfs.append(df_i)\n",
    "\n",
    "        df_yahoo = reduce(lambda x, y: pd.merge(x, y, how=join_method, on=['date', 'ticker']), list_of_dfs)\n",
    "        date_plus_ticker = df_yahoo['date'].astype(str) + df_yahoo['ticker'].astype(str) # one last quality check to ensure date + ticker is unique\n",
    "\n",
    "    return df_yahoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  100 of 100 completed\n",
      "[*********************100%***********************]  100 of 100 completed\n"
     ]
    }
   ],
   "source": [
    "df = download_yfinance_data(tickers2, num_workers=1, yfinance_params={'start':'2021-02-01', 'threads':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.4 s, sys: 33.6 s, total: 52 s\n",
      "Wall time: 31.3 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>yahoo_ticker</th>\n",
       "      <th>adj_close_1d</th>\n",
       "      <th>close_1d</th>\n",
       "      <th>high_1d</th>\n",
       "      <th>low_1d</th>\n",
       "      <th>open_1d</th>\n",
       "      <th>volume_1d</th>\n",
       "      <th>adj_close_1h_0</th>\n",
       "      <th>adj_close_1h_1</th>\n",
       "      <th>...</th>\n",
       "      <th>volume_1h_15</th>\n",
       "      <th>volume_1h_16</th>\n",
       "      <th>volume_1h_17</th>\n",
       "      <th>volume_1h_18</th>\n",
       "      <th>volume_1h_19</th>\n",
       "      <th>volume_1h_20</th>\n",
       "      <th>volume_1h_21</th>\n",
       "      <th>volume_1h_22</th>\n",
       "      <th>volume_1h_23</th>\n",
       "      <th>bloomberg_ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17616895</th>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>ZZZ.TO</td>\n",
       "      <td>31.879999</td>\n",
       "      <td>31.879999</td>\n",
       "      <td>32.240002</td>\n",
       "      <td>31.629999</td>\n",
       "      <td>31.98</td>\n",
       "      <td>1.107286e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>6800.0</td>\n",
       "      <td>3943.0</td>\n",
       "      <td>4128.0</td>\n",
       "      <td>6819.0</td>\n",
       "      <td>6188.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZZZ CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17616896</th>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>ZZZ.TO</td>\n",
       "      <td>31.879999</td>\n",
       "      <td>31.879999</td>\n",
       "      <td>32.240002</td>\n",
       "      <td>31.629999</td>\n",
       "      <td>31.98</td>\n",
       "      <td>1.107286e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>6800.0</td>\n",
       "      <td>3943.0</td>\n",
       "      <td>4128.0</td>\n",
       "      <td>6819.0</td>\n",
       "      <td>6188.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZZZ CN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 153 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               date yahoo_ticker  adj_close_1d   close_1d    high_1d  \\\n",
       "17616895 2021-04-06       ZZZ.TO     31.879999  31.879999  32.240002   \n",
       "17616896 2021-04-06       ZZZ.TO     31.879999  31.879999  32.240002   \n",
       "\n",
       "             low_1d  open_1d     volume_1d  adj_close_1h_0  adj_close_1h_1  \\\n",
       "17616895  31.629999    31.98  1.107286e+09             NaN             NaN   \n",
       "17616896  31.629999    31.98  1.107286e+09             NaN             NaN   \n",
       "\n",
       "          ...  volume_1h_15  volume_1h_16  volume_1h_17  volume_1h_18  \\\n",
       "17616895  ...        6800.0        3943.0        4128.0        6819.0   \n",
       "17616896  ...        6800.0        3943.0        4128.0        6819.0   \n",
       "\n",
       "          volume_1h_19  volume_1h_20  volume_1h_21  volume_1h_22  \\\n",
       "17616895        6188.0           NaN           NaN           NaN   \n",
       "17616896        6188.0           NaN           NaN           NaN   \n",
       "\n",
       "          volume_1h_23  bloomberg_ticker  \n",
       "17616895           NaN            ZZZ CN  \n",
       "17616896           NaN            ZZZ CN  \n",
       "\n",
       "[2 rows x 153 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "DOWNLOAD_YAHOO_DATA = False\n",
    "if DOWNLOAD_YAHOO_DATA:\n",
    "    df_yahoo = dd.from_pandas(download_yfinance_data(list(ticker_map['yahoo']), start='2006-01-01')) # all valid yahoo tickers\n",
    "else:\n",
    "    DF_YAHOO_FILEPATH = '/media/melgazar9/HDD_10TB/trading/data/yfinance/df_yahoo_2021-04-07.pq'\n",
    "    NPARTITIONS=16\n",
    "    if DF_YAHOO_FILEPATH.lower().endswith('pq') or DF_YAHOO_FILEPATH.lower().endswith('parquet'):\n",
    "        df_yahoo = dd.read_parquet(DF_YAHOO_FILEPATH,\n",
    "                                    npartitions=NPARTITIONS).compute()\n",
    "    elif DF_YAHOO_FILEPATH.lower().endswith('feather'):\n",
    "        df_yahoo = dd.from_pandas(delayed(feather.read_dataframe)(DF_YAHOO_FILEPATH).compute(),\n",
    "                                   npartitions=NPARTITIONS).compute()\n",
    "\n",
    "df_yahoo.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 17616897 entries, 0 to 17616896\n",
      "Columns: 153 entries, date to bloomberg_ticker\n",
      "dtypes: datetime64[ns](1), float64(150), object(2)\n",
      "memory usage: 20.2+ GB\n"
     ]
    }
   ],
   "source": [
    "df_yahoo.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the yahoo tickers to bloomberg tickers in the ddf_yahoo\n",
    "Set to True if downloading data. The mapping should already be saved in the dumped parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE_BLOOMBERG_TICKER_FROM_YAHOO = False\n",
    "if CREATE_BLOOMBERG_TICKER_FROM_YAHOO:\n",
    "    df_yahoo.loc[:, 'bloomberg_ticker'] = df_yahoo['yahoo_ticker'].map(dict(zip(ticker_map['yahoo'], ticker_map['bloomberg_ticker'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save df_yahoo to a feather or parquet file for faster loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13 µs, sys: 23 µs, total: 36 µs\n",
      "Wall time: 42.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "SAVE_DF_YAHOO_TO_FEATHER = False\n",
    "SAVE_DF_YAHOO_TO_PARQUET = False\n",
    "\n",
    "DF_YAHOO_OUTPATH = 'data/yfinance/df_yahoo_' + str(datetime.datetime.today().date())\n",
    "if SAVE_DF_YAHOO_TO_FEATHER:\n",
    "    df_yahoo.reset_index().to_feather(DF_YAHOO_OUTPATH + '.feather')\n",
    "if SAVE_DF_YAHOO_TO_PARQUET:\n",
    "    df_yahoo.to_parquet(DF_YAHOO_OUTPATH + '.pq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the numerai targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.3 s, sys: 602 ms, total: 1.91 s\n",
      "Wall time: 16.8 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bloomberg_ticker</th>\n",
       "      <th>friday_date</th>\n",
       "      <th>data_type</th>\n",
       "      <th>target</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4299721</th>\n",
       "      <td>ZYXI US</td>\n",
       "      <td>20210326</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2021-03-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4299722</th>\n",
       "      <td>ZZZ CN</td>\n",
       "      <td>20210326</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2021-03-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bloomberg_ticker  friday_date   data_type  target       date\n",
       "4299721          ZYXI US     20210326  validation     0.5 2021-03-26\n",
       "4299722           ZZZ CN     20210326  validation     0.5 2021-03-26"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# targets_address = 'https://numerai-signals-public-data.s3-us-west-2.amazonaws.com/signals_train_val.csv' # old\n",
    "targets_address = 'https://numerai-signals-public-data.s3-us-west-2.amazonaws.com/signals_train_val_bbg.csv'\n",
    "\n",
    "targets = pd.read_csv(targets_address)\\\n",
    "            .assign(date = lambda df: pd.to_datetime(df['friday_date'], format='%Y%m%d'))\n",
    "targets.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.50    2151203\n",
       " 0.25     859478\n",
       " 0.75     859032\n",
       " 1.00     215071\n",
       " 0.00     214939\n",
       " Name: target, dtype: int64,\n",
       " 0.50    0.500312\n",
       " 0.25    0.199891\n",
       " 0.75    0.199788\n",
       " 1.00    0.050020\n",
       " 0.00    0.049989\n",
       " Name: target, dtype: float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets['target'].value_counts(), targets['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17616897, 153)\n",
      "(0, 153)\n",
      "(17616897, 3)\n",
      "(17551417, 6)\n",
      "CPU times: user 10.2 s, sys: 2.06 s, total: 12.2 s\n",
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(df_yahoo.shape)\n",
    "print(df_yahoo.dropna().shape)\n",
    "print(df_yahoo.dropna(axis=1).shape)\n",
    "print(df_yahoo[[i for i in df_yahoo.columns if i.endswith('d')]].dropna().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First iteration (reduced dataset size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge targets into ddf_yahoo\n",
    "- From an inner join on `['date', 'bloomberg_ticker']` we lose about 85% of rows. <br>\n",
    "- If we drop rows with NAs we have 0 rows left no matter what. <br>\n",
    "- The best bet seems to be an outer join without dropping NA rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner Join\n",
    "- By doing an inner join we lose about 85% of the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.14 s, sys: 1.03 s, total: 6.17 s\n",
      "Wall time: 6.18 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>yahoo_ticker</th>\n",
       "      <th>adj_close_1d</th>\n",
       "      <th>close_1d</th>\n",
       "      <th>high_1d</th>\n",
       "      <th>low_1d</th>\n",
       "      <th>open_1d</th>\n",
       "      <th>volume_1d</th>\n",
       "      <th>adj_close_1h_0</th>\n",
       "      <th>adj_close_1h_1</th>\n",
       "      <th>...</th>\n",
       "      <th>volume_1h_18</th>\n",
       "      <th>volume_1h_19</th>\n",
       "      <th>volume_1h_20</th>\n",
       "      <th>volume_1h_21</th>\n",
       "      <th>volume_1h_22</th>\n",
       "      <th>volume_1h_23</th>\n",
       "      <th>bloomberg_ticker</th>\n",
       "      <th>friday_date</th>\n",
       "      <th>data_type</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2633674</th>\n",
       "      <td>2021-03-26</td>\n",
       "      <td>ZURN.SW</td>\n",
       "      <td>402.100006</td>\n",
       "      <td>402.100006</td>\n",
       "      <td>404.700012</td>\n",
       "      <td>400.899994</td>\n",
       "      <td>401.000000</td>\n",
       "      <td>512509.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZURN SW</td>\n",
       "      <td>20210326</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2633675</th>\n",
       "      <td>2021-03-26</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>14.880000</td>\n",
       "      <td>14.880000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>14.400000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>456200.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>65519.0</td>\n",
       "      <td>63185.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZYXI US</td>\n",
       "      <td>20210326</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2633676</th>\n",
       "      <td>2021-03-26</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>14.880000</td>\n",
       "      <td>14.880000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>14.400000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>456200.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>65519.0</td>\n",
       "      <td>63185.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZYXI US</td>\n",
       "      <td>20210326</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2633677</th>\n",
       "      <td>2021-03-26</td>\n",
       "      <td>ZZZ.TO</td>\n",
       "      <td>32.060001</td>\n",
       "      <td>32.060001</td>\n",
       "      <td>32.060001</td>\n",
       "      <td>31.625000</td>\n",
       "      <td>31.709999</td>\n",
       "      <td>40900.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5822.0</td>\n",
       "      <td>5578.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZZZ CN</td>\n",
       "      <td>20210326</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2633678</th>\n",
       "      <td>2021-03-26</td>\n",
       "      <td>ZZZ.TO</td>\n",
       "      <td>32.060001</td>\n",
       "      <td>32.060001</td>\n",
       "      <td>32.060001</td>\n",
       "      <td>31.625000</td>\n",
       "      <td>31.709999</td>\n",
       "      <td>40900.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5822.0</td>\n",
       "      <td>5578.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZZZ CN</td>\n",
       "      <td>20210326</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 156 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date yahoo_ticker  adj_close_1d    close_1d     high_1d  \\\n",
       "2633674 2021-03-26      ZURN.SW    402.100006  402.100006  404.700012   \n",
       "2633675 2021-03-26         ZYXI     14.880000   14.880000   15.500000   \n",
       "2633676 2021-03-26         ZYXI     14.880000   14.880000   15.500000   \n",
       "2633677 2021-03-26       ZZZ.TO     32.060001   32.060001   32.060001   \n",
       "2633678 2021-03-26       ZZZ.TO     32.060001   32.060001   32.060001   \n",
       "\n",
       "             low_1d     open_1d  volume_1d  adj_close_1h_0  adj_close_1h_1  \\\n",
       "2633674  400.899994  401.000000   512509.0             NaN             NaN   \n",
       "2633675   14.400000   15.500000   456200.0             NaN             NaN   \n",
       "2633676   14.400000   15.500000   456200.0             NaN             NaN   \n",
       "2633677   31.625000   31.709999    40900.0             NaN             NaN   \n",
       "2633678   31.625000   31.709999    40900.0             NaN             NaN   \n",
       "\n",
       "         ...  volume_1h_18  volume_1h_19  volume_1h_20  volume_1h_21  \\\n",
       "2633674  ...           NaN           NaN           NaN           NaN   \n",
       "2633675  ...       65519.0       63185.0           NaN           NaN   \n",
       "2633676  ...       65519.0       63185.0           NaN           NaN   \n",
       "2633677  ...        5822.0        5578.0           NaN           NaN   \n",
       "2633678  ...        5822.0        5578.0           NaN           NaN   \n",
       "\n",
       "         volume_1h_22  volume_1h_23  bloomberg_ticker  friday_date  \\\n",
       "2633674           NaN           NaN           ZURN SW     20210326   \n",
       "2633675           NaN           NaN           ZYXI US     20210326   \n",
       "2633676           NaN           NaN           ZYXI US     20210326   \n",
       "2633677           NaN           NaN            ZZZ CN     20210326   \n",
       "2633678           NaN           NaN            ZZZ CN     20210326   \n",
       "\n",
       "          data_type  target  \n",
       "2633674  validation    0.25  \n",
       "2633675  validation    0.50  \n",
       "2633676  validation    0.50  \n",
       "2633677  validation    0.50  \n",
       "2633678  validation    0.50  \n",
       "\n",
       "[5 rows x 156 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# print('Before: ', df_yahoo.shape[0].compute(), df_yahoo.shape[1])\n",
    "df_yahoo = pd.merge(df_yahoo, targets, on=['date', 'bloomberg_ticker'], how='inner')\n",
    "\n",
    "# print('After: ', df_yahoo.shape[0].compute(), df_yahoo.shape[1])\n",
    "df_yahoo.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.11 ms, sys: 0 ns, total: 5.11 ms\n",
      "Wall time: 4.58 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_yahoo.set_index('date', inplace=True)\n",
    "df_yahoo.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop rows where the daily prices are NA\n",
    "By dropping rows where the daily prices are NA we lose 0% rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_suffix_nas(df, col_suffix='1d', id_cols=['date', 'bloomberg_ticker']):\n",
    "    \n",
    "    df_ids = df[[col for col in df.columns \\\n",
    "                 if col.endswith(col_suffix) \\\n",
    "                 or col in id_cols]\\\n",
    "               ].dropna()[id_cols].isin(df[id_cols])\n",
    "    \n",
    "    df = df[df[id_cols].isin(df_ids[id_cols])]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 3 µs, total: 5 µs\n",
      "Wall time: 9.06 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "DROP_1D_NAS = False\n",
    "if DROP_1D_NAS:\n",
    "    df_yahoo = drop_suffix_nas(df_yahoo, col_suffix='1d')\n",
    "\n",
    "DROP_1H_NAS = False\n",
    "if DROP_1H_NAS:\n",
    "    df_yahoo = drop_suffix_nas(df_yahoo, col_suffix='1h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Features\n",
    "### Create naive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 134 ms, sys: 4.7 ms, total: 138 ms\n",
      "Wall time: 138 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1 HK',\n",
       " '000100 KS',\n",
       " '2 HK',\n",
       " '000210 KS',\n",
       " '000240 KS',\n",
       " '000270 KS',\n",
       " '3 HK',\n",
       " '4 HK',\n",
       " '6 HK',\n",
       " '000660 KS']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "TICKERS = df_yahoo['bloomberg_ticker'].unique().tolist()\n",
    "TICKERS[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_naive_features_single_symbol(df,\\\n",
    "                                        symbol='',\\\n",
    "                                        symbol_sep='',\\\n",
    "                                        open_col='open_1d',\\\n",
    "                                        high_col='high_1d',\\\n",
    "                                        low_col='low_1d',\\\n",
    "                                        close_col='adj_close_1d',\\\n",
    "                                        volume_col='volume_1d',\\\n",
    "                                        new_col_suffix='_1d',\\\n",
    "                                        copy=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    __________ \n",
    "    \n",
    "    df: Pandas-like / dask dataframe\n",
    "        For the stacked yfinance data used for numerai, the syntax is <groupby('bloomberg_ticker').apply(func)>\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if copy: df = df.copy()\n",
    "\n",
    "    df['move' + new_col_suffix] = df[close_col] - df[open_col]\n",
    "    df['move_pct' + new_col_suffix] = df['move' + new_col_suffix] / df[open_col]\n",
    "    df['move_pct_change' + new_col_suffix] = df['move' + new_col_suffix].pct_change()\n",
    "    df['open_minus_prev_close' + new_col_suffix] = df[open_col] - df[close_col].shift()\n",
    "    df['prev_close_pct_chg' + new_col_suffix] = df['move' + new_col_suffix] / df[close_col].shift()\n",
    "\n",
    "    df['range' + new_col_suffix] = df[high_col] - df[low_col]\n",
    "    df['range_pct_change' + new_col_suffix] = df['range' + new_col_suffix].pct_change()\n",
    "\n",
    "    df['high_move' + new_col_suffix] = df[high_col] - df[open_col]\n",
    "    df['high_move_pct' + new_col_suffix] = df['high_move' + new_col_suffix] / df[open_col]\n",
    "    df['high_move_pct_change' + new_col_suffix] = df['high_move' + new_col_suffix].pct_change()\n",
    "\n",
    "    df['low_move' + new_col_suffix] = df[low_col] - df[open_col]\n",
    "    df['low_move_pct' + new_col_suffix] = df['low_move' + new_col_suffix] / df[open_col]\n",
    "    df['low_move_pct_change' + new_col_suffix] = df['low_move' + new_col_suffix].pct_change()\n",
    "\n",
    "    df['volume_diff' + new_col_suffix] = df[volume_col] - df[volume_col].shift()\n",
    "    df['volume_pct_change' + new_col_suffix] = df[volume_col].pct_change()\n",
    "\n",
    "    df['close_minus_low' + new_col_suffix] = df[close_col] - df[low_col]\n",
    "    df['high_minus_close' + new_col_suffix] = df[high_col] - df[close_col]\n",
    "\n",
    "    df['prev_close_minus_low_minus' + new_col_suffix] = df[close_col].shift() - df[low_col]\n",
    "    df['high_minus_prev_close' + new_col_suffix] = df[high_col] - df[close_col].shift()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 7s, sys: 2.85 s, total: 1min 10s\n",
      "Wall time: 1min 10s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yahoo_ticker</th>\n",
       "      <th>adj_close_1d</th>\n",
       "      <th>close_1d</th>\n",
       "      <th>high_1d</th>\n",
       "      <th>low_1d</th>\n",
       "      <th>open_1d</th>\n",
       "      <th>volume_1d</th>\n",
       "      <th>adj_close_1h_0</th>\n",
       "      <th>adj_close_1h_1</th>\n",
       "      <th>adj_close_1h_2</th>\n",
       "      <th>...</th>\n",
       "      <th>high_move_pct_change_1d</th>\n",
       "      <th>low_move_1d</th>\n",
       "      <th>low_move_pct_1d</th>\n",
       "      <th>low_move_pct_change_1d</th>\n",
       "      <th>volume_diff_1d</th>\n",
       "      <th>volume_pct_change_1d</th>\n",
       "      <th>close_minus_low_1d</th>\n",
       "      <th>high_minus_close_1d</th>\n",
       "      <th>prev_close_minus_low_minus_1d</th>\n",
       "      <th>high_minus_prev_close_1d</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-03-12</th>\n",
       "      <td>ZZZ.TO</td>\n",
       "      <td>30.799999</td>\n",
       "      <td>30.799999</td>\n",
       "      <td>30.820000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.590000</td>\n",
       "      <td>123700.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.590000</td>\n",
       "      <td>-0.019287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.799999</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.799999</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-19</th>\n",
       "      <td>ZZZ.TO</td>\n",
       "      <td>31.730000</td>\n",
       "      <td>31.730000</td>\n",
       "      <td>31.885000</td>\n",
       "      <td>30.879999</td>\n",
       "      <td>31.270000</td>\n",
       "      <td>138800.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.673917</td>\n",
       "      <td>-0.390001</td>\n",
       "      <td>-0.012472</td>\n",
       "      <td>-0.338981</td>\n",
       "      <td>15100.0</td>\n",
       "      <td>0.122070</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.155001</td>\n",
       "      <td>-0.080000</td>\n",
       "      <td>1.085001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-19</th>\n",
       "      <td>ZZZ.TO</td>\n",
       "      <td>31.730000</td>\n",
       "      <td>31.730000</td>\n",
       "      <td>31.885000</td>\n",
       "      <td>30.879999</td>\n",
       "      <td>31.270000</td>\n",
       "      <td>138800.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.390001</td>\n",
       "      <td>-0.012472</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.155001</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.155001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-26</th>\n",
       "      <td>ZZZ.TO</td>\n",
       "      <td>32.060001</td>\n",
       "      <td>32.060001</td>\n",
       "      <td>32.060001</td>\n",
       "      <td>31.625000</td>\n",
       "      <td>31.709999</td>\n",
       "      <td>40900.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.430890</td>\n",
       "      <td>-0.084999</td>\n",
       "      <td>-0.002681</td>\n",
       "      <td>-0.782054</td>\n",
       "      <td>-97900.0</td>\n",
       "      <td>-0.705331</td>\n",
       "      <td>0.435001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105000</td>\n",
       "      <td>0.330002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-26</th>\n",
       "      <td>ZZZ.TO</td>\n",
       "      <td>32.060001</td>\n",
       "      <td>32.060001</td>\n",
       "      <td>32.060001</td>\n",
       "      <td>31.625000</td>\n",
       "      <td>31.709999</td>\n",
       "      <td>40900.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.084999</td>\n",
       "      <td>-0.002681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.435001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.435001</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 174 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           yahoo_ticker  adj_close_1d   close_1d    high_1d     low_1d  \\\n",
       "date                                                                     \n",
       "2021-03-12       ZZZ.TO     30.799999  30.799999  30.820000  30.000000   \n",
       "2021-03-19       ZZZ.TO     31.730000  31.730000  31.885000  30.879999   \n",
       "2021-03-19       ZZZ.TO     31.730000  31.730000  31.885000  30.879999   \n",
       "2021-03-26       ZZZ.TO     32.060001  32.060001  32.060001  31.625000   \n",
       "2021-03-26       ZZZ.TO     32.060001  32.060001  32.060001  31.625000   \n",
       "\n",
       "              open_1d  volume_1d  adj_close_1h_0  adj_close_1h_1  \\\n",
       "date                                                               \n",
       "2021-03-12  30.590000   123700.0             NaN             NaN   \n",
       "2021-03-19  31.270000   138800.0             NaN             NaN   \n",
       "2021-03-19  31.270000   138800.0             NaN             NaN   \n",
       "2021-03-26  31.709999    40900.0             NaN             NaN   \n",
       "2021-03-26  31.709999    40900.0             NaN             NaN   \n",
       "\n",
       "            adj_close_1h_2  ...  high_move_pct_change_1d  low_move_1d  \\\n",
       "date                        ...                                         \n",
       "2021-03-12             NaN  ...                 0.000000    -0.590000   \n",
       "2021-03-19             NaN  ...                 1.673917    -0.390001   \n",
       "2021-03-19             NaN  ...                 0.000000    -0.390001   \n",
       "2021-03-26             NaN  ...                -0.430890    -0.084999   \n",
       "2021-03-26             NaN  ...                 0.000000    -0.084999   \n",
       "\n",
       "            low_move_pct_1d  low_move_pct_change_1d  volume_diff_1d  \\\n",
       "date                                                                  \n",
       "2021-03-12        -0.019287                0.000000             0.0   \n",
       "2021-03-19        -0.012472               -0.338981         15100.0   \n",
       "2021-03-19        -0.012472                0.000000             0.0   \n",
       "2021-03-26        -0.002681               -0.782054        -97900.0   \n",
       "2021-03-26        -0.002681                0.000000             0.0   \n",
       "\n",
       "            volume_pct_change_1d  close_minus_low_1d  high_minus_close_1d  \\\n",
       "date                                                                        \n",
       "2021-03-12              0.000000            0.799999             0.020000   \n",
       "2021-03-19              0.122070            0.850000             0.155001   \n",
       "2021-03-19              0.000000            0.850000             0.155001   \n",
       "2021-03-26             -0.705331            0.435001             0.000000   \n",
       "2021-03-26              0.000000            0.435001             0.000000   \n",
       "\n",
       "            prev_close_minus_low_minus_1d  high_minus_prev_close_1d  \n",
       "date                                                                 \n",
       "2021-03-12                       0.799999                  0.020000  \n",
       "2021-03-19                      -0.080000                  1.085001  \n",
       "2021-03-19                       0.850000                  0.155001  \n",
       "2021-03-26                       0.105000                  0.330002  \n",
       "2021-03-26                       0.435001                  0.000000  \n",
       "\n",
       "[5 rows x 174 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_yahoo = df_yahoo.groupby('bloomberg_ticker', group_keys=False).apply(create_naive_features_single_symbol)\n",
    "df_yahoo.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create my own rule based targets as a feature\n",
    "These are things that I would be looking for before I make a trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateTargets():\n",
    "\n",
    "    def __init__(self, df, copy = True):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        __________\n",
    "\n",
    "        df : pandas df\n",
    "        copy : Boolean whether to make a copy of the df before applying transformations\n",
    "        \n",
    "        Note: to compute the target based on pct, pass the pct column names into the individual functions\n",
    "        \"\"\"\n",
    "\n",
    "        self.df = df\n",
    "        self.copy = copy\n",
    "\n",
    "        if self.copy: self.df = self.df.copy()\n",
    "\n",
    "    def create_targets_HL5(self,\\\n",
    "                           strong_buy,\\\n",
    "                           med_buy,\\\n",
    "                           med_sell,\\\n",
    "                           strong_sell,\\\n",
    "                           threshold,\\\n",
    "                           stop,\\\n",
    "                           move_col = 'move_pct',\\\n",
    "                           lm_col = 'low_move_pct',\\\n",
    "                           hm_col = 'high_move_pct',\\\n",
    "                           target_suffix = 'target_HL5'):\n",
    "\n",
    "\n",
    "        # hm stands for high move, lm stands for low move\n",
    "        # Strong Buy\n",
    "        self.df.loc[(self.df[hm_col] >= strong_buy) &\\\n",
    "                            (self.df[lm_col] >= (-1)*stop),\\\n",
    "                            target_suffix] = 4\n",
    "\n",
    "        # Strong Sell\n",
    "        self.df.loc[(self.df[lm_col] <= (-1)*strong_sell) &\\\n",
    "                    (self.df[hm_col] <= stop) &\\\n",
    "                    (self.df[target_suffix] != 4),\\\n",
    "                    target_suffix] = 0\n",
    "\n",
    "        # Medium Buy\n",
    "        self.df.loc[(self.df[hm_col] >= med_buy) &\\\n",
    "                            (self.df[lm_col] >= (-1)*stop) &\\\n",
    "                            (self.df[target_suffix] != 4) &\\\n",
    "                            (self.df[target_suffix] != 0),\\\n",
    "                            target_suffix] = 3\n",
    "\n",
    "        # Medium Sell\n",
    "        self.df.loc[(self.df[lm_col] <= (-1)*med_sell) &\\\n",
    "                            (self.df[hm_col] <= stop) &\\\n",
    "                            (self.df[target_suffix] != 4) &\\\n",
    "                            (self.df[target_suffix] != 0) &\\\n",
    "                            (self.df[target_suffix] != 3),\\\n",
    "                            target_suffix] = 1\n",
    "\n",
    "        # No Trade\n",
    "        self.df.loc[(self.df[target_suffix] != 0) &\\\n",
    "                            (self.df[target_suffix] != 1) &\\\n",
    "                            (self.df[target_suffix] != 3) &\\\n",
    "                            (self.df[target_suffix] != 4),\\\n",
    "                            target_suffix] = 2\n",
    "\n",
    "\n",
    "        return self.df\n",
    "\n",
    "\n",
    "    def create_targets_HL3(self,\\\n",
    "                           buy,\\\n",
    "                           sell,\\\n",
    "                           threshold,\\\n",
    "                           stop,\\\n",
    "                           move_col = 'move_pct',\\\n",
    "                           lm_col = 'low_move_pct',\\\n",
    "                           hm_col = 'high_move_pct',\\\n",
    "                           target_suffix = 'target_HL3'):\n",
    "\n",
    "\n",
    "        # hm stands for high move, lm stands for low move\n",
    "        # Buy\n",
    "        self.df.loc[(self.df[hm_col] >= buy) &\\\n",
    "                            (self.df[lm_col] >= (-1)*stop),\\\n",
    "                            target_suffix] = 2\n",
    "\n",
    "        # Sell\n",
    "        self.df.loc[(self.df[lm_col] <= (-1)*sell) &\\\n",
    "                            (self.df[hm_col] <= stop) &\\\n",
    "                            (self.df[target_suffix] != 2),\\\n",
    "                            target_suffix] = 0\n",
    "\n",
    "        # No Trade\n",
    "        self.df.loc[(self.df[target_suffix] != 0) &\\\n",
    "                            (self.df[target_suffix] != 2),\\\n",
    "                            target_suffix] = 1\n",
    "\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 378 ms, sys: 288 ms, total: 666 ms\n",
      "Wall time: 666 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_yahoo = CreateTargets(df_yahoo).create_targets_HL3(buy=0.03,\\\n",
    "                                                      sell=0.03,\\\n",
    "                                                      threshold=0.25,\\\n",
    "                                                      stop=.01,\\\n",
    "                                                      move_col = 'move_pct_1d',\\\n",
    "                                                      lm_col = 'low_move_pct_1d',\\\n",
    "                                                      hm_col = 'high_move_pct_1d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    2224340\n",
       "0.0     213419\n",
       "2.0     195920\n",
       "Name: target_HL3, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.0    0.844575\n",
       "0.0    0.081035\n",
       "2.0    0.074390\n",
       "Name: target_HL3, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(df_yahoo['target_HL3'].value_counts()), display(df_yahoo['target_HL3'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 460 ms, sys: 285 ms, total: 745 ms\n",
      "Wall time: 746 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yahoo_ticker</th>\n",
       "      <th>adj_close_1d</th>\n",
       "      <th>close_1d</th>\n",
       "      <th>high_1d</th>\n",
       "      <th>low_1d</th>\n",
       "      <th>open_1d</th>\n",
       "      <th>volume_1d</th>\n",
       "      <th>adj_close_1h_0</th>\n",
       "      <th>adj_close_1h_1</th>\n",
       "      <th>adj_close_1h_2</th>\n",
       "      <th>...</th>\n",
       "      <th>low_move_pct_1d</th>\n",
       "      <th>low_move_pct_change_1d</th>\n",
       "      <th>volume_diff_1d</th>\n",
       "      <th>volume_pct_change_1d</th>\n",
       "      <th>close_minus_low_1d</th>\n",
       "      <th>high_minus_close_1d</th>\n",
       "      <th>prev_close_minus_low_minus_1d</th>\n",
       "      <th>high_minus_prev_close_1d</th>\n",
       "      <th>target_HL3</th>\n",
       "      <th>target_HL5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-03-12</th>\n",
       "      <td>ZZZ.TO</td>\n",
       "      <td>30.799999</td>\n",
       "      <td>30.799999</td>\n",
       "      <td>30.820000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.590000</td>\n",
       "      <td>123700.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.799999</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.799999</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-19</th>\n",
       "      <td>ZZZ.TO</td>\n",
       "      <td>31.730000</td>\n",
       "      <td>31.730000</td>\n",
       "      <td>31.885000</td>\n",
       "      <td>30.879999</td>\n",
       "      <td>31.270000</td>\n",
       "      <td>138800.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012472</td>\n",
       "      <td>-0.338981</td>\n",
       "      <td>15100.0</td>\n",
       "      <td>0.122070</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.155001</td>\n",
       "      <td>-0.080000</td>\n",
       "      <td>1.085001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-19</th>\n",
       "      <td>ZZZ.TO</td>\n",
       "      <td>31.730000</td>\n",
       "      <td>31.730000</td>\n",
       "      <td>31.885000</td>\n",
       "      <td>30.879999</td>\n",
       "      <td>31.270000</td>\n",
       "      <td>138800.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012472</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.155001</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.155001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-26</th>\n",
       "      <td>ZZZ.TO</td>\n",
       "      <td>32.060001</td>\n",
       "      <td>32.060001</td>\n",
       "      <td>32.060001</td>\n",
       "      <td>31.625000</td>\n",
       "      <td>31.709999</td>\n",
       "      <td>40900.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002681</td>\n",
       "      <td>-0.782054</td>\n",
       "      <td>-97900.0</td>\n",
       "      <td>-0.705331</td>\n",
       "      <td>0.435001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105000</td>\n",
       "      <td>0.330002</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-26</th>\n",
       "      <td>ZZZ.TO</td>\n",
       "      <td>32.060001</td>\n",
       "      <td>32.060001</td>\n",
       "      <td>32.060001</td>\n",
       "      <td>31.625000</td>\n",
       "      <td>31.709999</td>\n",
       "      <td>40900.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.435001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.435001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 176 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           yahoo_ticker  adj_close_1d   close_1d    high_1d     low_1d  \\\n",
       "date                                                                     \n",
       "2021-03-12       ZZZ.TO     30.799999  30.799999  30.820000  30.000000   \n",
       "2021-03-19       ZZZ.TO     31.730000  31.730000  31.885000  30.879999   \n",
       "2021-03-19       ZZZ.TO     31.730000  31.730000  31.885000  30.879999   \n",
       "2021-03-26       ZZZ.TO     32.060001  32.060001  32.060001  31.625000   \n",
       "2021-03-26       ZZZ.TO     32.060001  32.060001  32.060001  31.625000   \n",
       "\n",
       "              open_1d  volume_1d  adj_close_1h_0  adj_close_1h_1  \\\n",
       "date                                                               \n",
       "2021-03-12  30.590000   123700.0             NaN             NaN   \n",
       "2021-03-19  31.270000   138800.0             NaN             NaN   \n",
       "2021-03-19  31.270000   138800.0             NaN             NaN   \n",
       "2021-03-26  31.709999    40900.0             NaN             NaN   \n",
       "2021-03-26  31.709999    40900.0             NaN             NaN   \n",
       "\n",
       "            adj_close_1h_2  ...  low_move_pct_1d  low_move_pct_change_1d  \\\n",
       "date                        ...                                            \n",
       "2021-03-12             NaN  ...        -0.019287                0.000000   \n",
       "2021-03-19             NaN  ...        -0.012472               -0.338981   \n",
       "2021-03-19             NaN  ...        -0.012472                0.000000   \n",
       "2021-03-26             NaN  ...        -0.002681               -0.782054   \n",
       "2021-03-26             NaN  ...        -0.002681                0.000000   \n",
       "\n",
       "            volume_diff_1d  volume_pct_change_1d  close_minus_low_1d  \\\n",
       "date                                                                   \n",
       "2021-03-12             0.0              0.000000            0.799999   \n",
       "2021-03-19         15100.0              0.122070            0.850000   \n",
       "2021-03-19             0.0              0.000000            0.850000   \n",
       "2021-03-26        -97900.0             -0.705331            0.435001   \n",
       "2021-03-26             0.0              0.000000            0.435001   \n",
       "\n",
       "            high_minus_close_1d  prev_close_minus_low_minus_1d  \\\n",
       "date                                                             \n",
       "2021-03-12             0.020000                       0.799999   \n",
       "2021-03-19             0.155001                      -0.080000   \n",
       "2021-03-19             0.155001                       0.850000   \n",
       "2021-03-26             0.000000                       0.105000   \n",
       "2021-03-26             0.000000                       0.435001   \n",
       "\n",
       "            high_minus_prev_close_1d  target_HL3  target_HL5  \n",
       "date                                                          \n",
       "2021-03-12                  0.020000         1.0         1.0  \n",
       "2021-03-19                  1.085001         1.0         3.0  \n",
       "2021-03-19                  0.155001         1.0         3.0  \n",
       "2021-03-26                  0.330002         1.0         2.0  \n",
       "2021-03-26                  0.000000         1.0         2.0  \n",
       "\n",
       "[5 rows x 176 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_yahoo = CreateTargets(df_yahoo).create_targets_HL5(strong_buy=0.035,\\\n",
    "                                                      med_buy=0.015,\\\n",
    "                                                      med_sell=0.015,\\\n",
    "                                                      strong_sell=0.035,\\\n",
    "                                                      threshold=0.25,\\\n",
    "                                                      stop=.025,\\\n",
    "                                                      move_col = 'move_pct_1d',\\\n",
    "                                                      lm_col = 'low_move_pct_1d',\\\n",
    "                                                      hm_col = 'high_move_pct_1d')\n",
    "df_yahoo.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    1069661\n",
       "3.0     585406\n",
       "1.0     585000\n",
       "0.0     201187\n",
       "4.0     192425\n",
       "Name: target_HL5, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2.0    0.406147\n",
       "3.0    0.222277\n",
       "1.0    0.222123\n",
       "0.0    0.076390\n",
       "4.0    0.073063\n",
       "Name: target_HL5, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(df_yahoo['target_HL5'].value_counts()), display(df_yahoo['target_HL5'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create some more features before applying preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lagging_features(df, lagging_map, groupby_cols=None, new_col_prefix='prev', copy=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    __________\n",
    "    \n",
    "    df : pandas df\n",
    "    groupby_cols : str or list of cols to groupby before creating lagging transformation cols\n",
    "    lagging_map : dict with keys as colnames and values as a list of periods for computing lagging features\n",
    "    periods : periods to look back\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if copy: df = df.copy()\n",
    "\n",
    "    unique_lagging_values = list(sorted({k for v in lagging_map.values() for k in v}))\n",
    "    \n",
    "    if groupby_cols is None or len(groupby_cols) == 0:\n",
    "        for period in unique_lagging_values:\n",
    "            new_col_prefix_tmp = new_col_prefix + str(period) + '_'\n",
    "            cols_to_lag = [k for k,v in lagging_map.items() if period in v]\n",
    "            df[[new_col_prefix_tmp + c for c in cols_to_lag]] = df[cols_to_lag].transform(lambda s: s.shift(periods=period))\n",
    "    \n",
    "    else:\n",
    "        for period in unique_lagging_values:\n",
    "            new_col_prefix_tmp = new_col_prefix + str(period) + '_'\n",
    "            cols_to_lag = [k for k,v in lagging_map.items() if period in v]\n",
    "            \n",
    "            df[[new_col_prefix_tmp + c for c in cols_to_lag]] = df.groupby(groupby_cols)[cols_to_lag]\\\n",
    "                                                                  .transform(lambda s: s.shift(periods=period))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': [1, 2, 3, 4, 5],\n",
       " 'target_HL5': [1, 2, 3, 4, 5],\n",
       " 'volume_1d': [1, 2, 3, 4, 5],\n",
       " 'adj_close_1d': [1, 2, 3, 4, 5],\n",
       " 'move_1d': [1, 2, 3, 4, 5]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LAGGING_MAP = {'target': [1, 2, 3, 4, 5],\\\n",
    "               'target_HL5': [1, 2, 3, 4, 5],\\\n",
    "               'volume_1d': [1, 2, 3, 4, 5],\\\n",
    "               'adj_close_1d' : [1, 2, 3, 4, 5],\\\n",
    "               'move_1d':[1,2,3,4,5]}\n",
    "LAGGING_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.9 s, sys: 4.17 s, total: 46.1 s\n",
      "Wall time: 46.1 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yahoo_ticker</th>\n",
       "      <th>adj_close_1d</th>\n",
       "      <th>close_1d</th>\n",
       "      <th>high_1d</th>\n",
       "      <th>low_1d</th>\n",
       "      <th>open_1d</th>\n",
       "      <th>volume_1d</th>\n",
       "      <th>adj_close_1h_0</th>\n",
       "      <th>adj_close_1h_1</th>\n",
       "      <th>adj_close_1h_2</th>\n",
       "      <th>...</th>\n",
       "      <th>prev4_target</th>\n",
       "      <th>prev4_target_HL5</th>\n",
       "      <th>prev4_volume_1d</th>\n",
       "      <th>prev4_adj_close_1d</th>\n",
       "      <th>prev4_move_1d</th>\n",
       "      <th>prev5_target</th>\n",
       "      <th>prev5_target_HL5</th>\n",
       "      <th>prev5_volume_1d</th>\n",
       "      <th>prev5_adj_close_1d</th>\n",
       "      <th>prev5_move_1d</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-03-12</th>\n",
       "      <td>ZZZ.TO</td>\n",
       "      <td>30.799999</td>\n",
       "      <td>30.799999</td>\n",
       "      <td>30.820000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.590000</td>\n",
       "      <td>123700.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3.0</td>\n",
       "      <td>106100.0</td>\n",
       "      <td>26.930000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3.0</td>\n",
       "      <td>106100.0</td>\n",
       "      <td>26.930000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-19</th>\n",
       "      <td>ZZZ.TO</td>\n",
       "      <td>31.730000</td>\n",
       "      <td>31.730000</td>\n",
       "      <td>31.885000</td>\n",
       "      <td>30.879999</td>\n",
       "      <td>31.270000</td>\n",
       "      <td>138800.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>340600.0</td>\n",
       "      <td>30.549999</td>\n",
       "      <td>-0.380001</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3.0</td>\n",
       "      <td>106100.0</td>\n",
       "      <td>26.930000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-19</th>\n",
       "      <td>ZZZ.TO</td>\n",
       "      <td>31.730000</td>\n",
       "      <td>31.730000</td>\n",
       "      <td>31.885000</td>\n",
       "      <td>30.879999</td>\n",
       "      <td>31.270000</td>\n",
       "      <td>138800.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>340600.0</td>\n",
       "      <td>30.549999</td>\n",
       "      <td>-0.380001</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>340600.0</td>\n",
       "      <td>30.549999</td>\n",
       "      <td>-0.380001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-26</th>\n",
       "      <td>ZZZ.TO</td>\n",
       "      <td>32.060001</td>\n",
       "      <td>32.060001</td>\n",
       "      <td>32.060001</td>\n",
       "      <td>31.625000</td>\n",
       "      <td>31.709999</td>\n",
       "      <td>40900.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>123700.0</td>\n",
       "      <td>30.799999</td>\n",
       "      <td>0.209999</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>340600.0</td>\n",
       "      <td>30.549999</td>\n",
       "      <td>-0.380001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-26</th>\n",
       "      <td>ZZZ.TO</td>\n",
       "      <td>32.060001</td>\n",
       "      <td>32.060001</td>\n",
       "      <td>32.060001</td>\n",
       "      <td>31.625000</td>\n",
       "      <td>31.709999</td>\n",
       "      <td>40900.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>123700.0</td>\n",
       "      <td>30.799999</td>\n",
       "      <td>0.209999</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>123700.0</td>\n",
       "      <td>30.799999</td>\n",
       "      <td>0.209999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           yahoo_ticker  adj_close_1d   close_1d    high_1d     low_1d  \\\n",
       "date                                                                     \n",
       "2021-03-12       ZZZ.TO     30.799999  30.799999  30.820000  30.000000   \n",
       "2021-03-19       ZZZ.TO     31.730000  31.730000  31.885000  30.879999   \n",
       "2021-03-19       ZZZ.TO     31.730000  31.730000  31.885000  30.879999   \n",
       "2021-03-26       ZZZ.TO     32.060001  32.060001  32.060001  31.625000   \n",
       "2021-03-26       ZZZ.TO     32.060001  32.060001  32.060001  31.625000   \n",
       "\n",
       "              open_1d  volume_1d  adj_close_1h_0  adj_close_1h_1  \\\n",
       "date                                                               \n",
       "2021-03-12  30.590000   123700.0             NaN             NaN   \n",
       "2021-03-19  31.270000   138800.0             NaN             NaN   \n",
       "2021-03-19  31.270000   138800.0             NaN             NaN   \n",
       "2021-03-26  31.709999    40900.0             NaN             NaN   \n",
       "2021-03-26  31.709999    40900.0             NaN             NaN   \n",
       "\n",
       "            adj_close_1h_2  ...  prev4_target  prev4_target_HL5  \\\n",
       "date                        ...                                   \n",
       "2021-03-12             NaN  ...          0.75               3.0   \n",
       "2021-03-19             NaN  ...          0.50               1.0   \n",
       "2021-03-19             NaN  ...          0.50               1.0   \n",
       "2021-03-26             NaN  ...          0.75               1.0   \n",
       "2021-03-26             NaN  ...          0.75               1.0   \n",
       "\n",
       "            prev4_volume_1d  prev4_adj_close_1d  prev4_move_1d  prev5_target  \\\n",
       "date                                                                           \n",
       "2021-03-12         106100.0           26.930000       0.600000          0.75   \n",
       "2021-03-19         340600.0           30.549999      -0.380001          0.75   \n",
       "2021-03-19         340600.0           30.549999      -0.380001          0.50   \n",
       "2021-03-26         123700.0           30.799999       0.209999          0.50   \n",
       "2021-03-26         123700.0           30.799999       0.209999          0.75   \n",
       "\n",
       "            prev5_target_HL5  prev5_volume_1d  prev5_adj_close_1d  \\\n",
       "date                                                                \n",
       "2021-03-12               3.0         106100.0           26.930000   \n",
       "2021-03-19               3.0         106100.0           26.930000   \n",
       "2021-03-19               1.0         340600.0           30.549999   \n",
       "2021-03-26               1.0         340600.0           30.549999   \n",
       "2021-03-26               1.0         123700.0           30.799999   \n",
       "\n",
       "            prev5_move_1d  \n",
       "date                       \n",
       "2021-03-12       0.600000  \n",
       "2021-03-19       0.600000  \n",
       "2021-03-19      -0.380001  \n",
       "2021-03-26      -0.380001  \n",
       "2021-03-26       0.209999  \n",
       "\n",
       "[5 rows x 201 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_yahoo = create_lagging_features(df_yahoo, groupby_cols='bloomberg_ticker', lagging_map=LAGGING_MAP)\n",
    "df_yahoo.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create rolling features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rolling_features(df,\\\n",
    "                            rolling_fn,\\\n",
    "                            rolling_params,\\\n",
    "                            ewm_fn,\\\n",
    "                            ewm_params,\\\n",
    "                            rolling_cols = 'all_numeric',\\\n",
    "                            ewm_cols='all_numeric',\\\n",
    "                            join_method='outer',\\\n",
    "                            groupby_cols=None,\\\n",
    "                            create_diff_cols=True,\n",
    "                            copy=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    __________\n",
    "    df : pandas df\n",
    "    \n",
    "    rolling_fn : str called from df.rolling().rolling_fn (e.g. df.rolling.mean() is called with getattr)\n",
    "    rolling_params : dict params passed to df.rolling()\n",
    "    \n",
    "    ewm_fn : str called from df.ewm().ewm_fn (e.g. df.ewm.mean() is called with getattr)\n",
    "    ewm_params : dict params passed to df.ewm()\n",
    "    \n",
    "    rolling_cols : cols to apply rolling_fn\n",
    "    ewm_cols : cols to apply ewm_fn\n",
    "    \n",
    "    join_method : str 'inner', 'outer', 'left', or 'right' - how to join the dfs\n",
    "    groupby_cols : list or str cols to group by before applying rolling transformations\n",
    "        example: pass groupby_cols to the stacked ticker numerai dataset, but not a wide df \n",
    "    \n",
    "    copy : bool whether or not to make a copy of the df\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if copy: df = df.copy()\n",
    "    \n",
    "    if isinstance(rolling_cols, str) and rolling_cols.lower() == 'all_numeric':\n",
    "        rolling_cols = list(df.select_dtypes(include=np.number).columns)\n",
    "    \n",
    "    if isinstance(rolling_cols, str) and ewm_cols.lower() == 'all_numeric':\n",
    "        ewm_cols = list(df.select_dtypes(include=np.number).columns)\n",
    "    \n",
    "    lag_dfs_lst = []\n",
    "    \n",
    "    if groupby_cols is None or len(groupby_cols) == 0:\n",
    "        \n",
    "        # rolling\n",
    "        lag_dfs_lst.append(getattr(df[rolling_cols].rolling(**rolling_params), rolling_fn)().add_suffix('_rolling_' + rolling_fn))\n",
    "        \n",
    "        # ewm\n",
    "        lag_dfs_lst.append(getattr(df[ewm_cols].ewm(**ewm_params), ewm_fn)().add_suffix('_ewm_' + ewm_fn))\n",
    "    else:\n",
    "        \n",
    "        if isinstance(groupby_cols, list):\n",
    "            assert(len(groupby_cols) == len(set(groupby_cols))), 'There are duplicates in groupby_cols!'\n",
    "            rolling_cols_to_select = [i for i in list(set(groupby_cols + rolling_cols)) if i in df.columns] # could be index name\n",
    "            ewm_cols_to_select = [i for i in list(set(groupby_cols + ewm_cols)) if i in df.columns] # could be index name\n",
    "        elif isinstance(groupby_cols, str):\n",
    "            rolling_cols_to_select = [i for i in list(set([groupby_cols] + rolling_cols)) if i in df.columns]\n",
    "            ewm_cols_to_select = [i for i in list(set([groupby_cols] + ewm_cols)) if i in df.columns]\n",
    "        else:\n",
    "            raise('Input param groupby_cols is not a list, string, or None!')\n",
    "        \n",
    "        # rolling\n",
    "        lag_dfs_lst.append(\n",
    "            df[rolling_cols_to_select].\\\n",
    "            groupby(groupby_cols).\\\n",
    "            apply(lambda x: getattr(x.rolling(**rolling_params), rolling_fn)()).\\\n",
    "            add_suffix('_rolling_' + rolling_fn)\\\n",
    "        )\n",
    "        \n",
    "        # ewm\n",
    "        lag_dfs_lst.append(\n",
    "            df[ewm_cols_to_select].\\\n",
    "            groupby(groupby_cols).\\\n",
    "            apply(lambda x: getattr(x.ewm(**ewm_params), ewm_fn)()).\\\n",
    "            add_suffix('_ewm_' + ewm_fn)\\\n",
    "        )\n",
    "\n",
    "    df_lag = reduce(lambda x, y: pd.merge(x, y, how=join_method, left_index=True, right_index=True), lag_dfs_lst)    \n",
    "    del lag_dfs_lst\n",
    "    df = pd.merge(df, df_lag, how=join_method, left_index=True, right_index=True)\n",
    "    \n",
    "    del df_lag\n",
    "    \n",
    "    if create_diff_cols:\n",
    "        if groupby_cols is None or len(groupby_cols) == 0:\n",
    "            df = pd.concat([df, df[[i for i in df.columns if 'ewm' in i or 'rolling' in i]].diff().add_suffix('_diff')], axis=1)\n",
    "        else:\n",
    "            diff_cols = [i for i in df.columns if 'ewm' in i or 'rolling' in i]\n",
    "            df[[i + '_diff' for i in diff_cols]] = df.groupby(groupby_cols)[diff_cols].transform(lambda col: col.diff())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Below will use over 130gb of ram if running through jupyter notebook. This notebook will be converted to a py script, which is less memory greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_yahoo = create_rolling_features(df_yahoo,\\\n",
    "                                   rolling_params={'window':30},\\\n",
    "                                   rolling_fn='mean',\\\n",
    "                                   ewm_params={'com':.5},\\\n",
    "                                   ewm_fn='mean',\\\n",
    "                                   rolling_cols = ['open_1d', 'high_1d', 'low_1d', 'adj_close_1d', 'volume_1d', 'prev1_target', 'prev1_target_HL5'],\\\n",
    "                                   ewm_cols = ['open_1d', 'high_1d', 'low_1d', 'adj_close_1d', 'volume_1d', 'prev1_target', 'prev1_target_HL5'],\\\n",
    "                                   join_method='outer',\\\n",
    "                                   groupby_cols = 'bloomberg_ticker',\\\n",
    "                                   create_diff_cols=True)\n",
    "df_yahoo.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a good checkpoint to save the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yahoo.reset_index(drop=True).to_feather(OUTPUT_PATH + 'df_numerai_' + str(datetime.datetime.today().date()) + '.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>price_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>127</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>131</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>133</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>132</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>130</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>130</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>128</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>127</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>126</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>129</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    price  price_diff\n",
       "0     127         NaN\n",
       "1     128         1.0\n",
       "2     131         3.0\n",
       "3     132         1.0\n",
       "4     133         1.0\n",
       "5     132        -1.0\n",
       "6     130        -2.0\n",
       "7     130         0.0\n",
       "8     128        -2.0\n",
       "9     127        -1.0\n",
       "10    126        -1.0\n",
       "11    129         3.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'price': [127, 128, 131, 132, 133, 132, 130, 130, 128, 127, 126, 129]})\n",
    "df['price_diff'] = df['price'].diff()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_move_iar(df, iar_cols, iar_suffix='_iar', copy=True):\n",
    "\n",
    "    if copy: df = df.copy()\n",
    "\n",
    "    tmp1 = df[iar_cols].transform(lambda x: x.cumsum().sub(x.cumsum().mask(x >= 0).ffill(), fill_value=0), axis=0).replace(0, np.nan)\n",
    "    tmp2 = df[iar_cols].transform(lambda x: x.cumsum().sub(x.cumsum().mask(x <= 0).ffill(), fill_value=0), axis=0).replace(0, np.nan)\n",
    "    \n",
    "    assert isinstance(iar_cols, str) or isinstance(iar_cols, list), 'iar_cols must be a str or list!'\n",
    "    \n",
    "    if isinstance(iar_cols, str):\n",
    "        df[iar_cols + iar_suffix] = tmp1.fillna(tmp2).ffill()\n",
    "    else:\n",
    "        df[[i + iar_suffix for i in iar_cols]] = tmp1.fillna(tmp2).ffill()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>price_diff</th>\n",
       "      <th>price_diff_iar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>127</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>131</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>133</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>132</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>130</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>128</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>127</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>126</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>129</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    price  price_diff  price_diff_iar\n",
       "0     127         NaN             NaN\n",
       "1     128         1.0             1.0\n",
       "2     131         3.0             4.0\n",
       "3     132         1.0             5.0\n",
       "4     133         1.0             6.0\n",
       "5     132        -1.0            -1.0\n",
       "6     130        -2.0            -3.0\n",
       "7     130         0.0            -3.0\n",
       "8     128        -2.0            -5.0\n",
       "9     127        -1.0            -6.0\n",
       "10    126        -1.0            -7.0\n",
       "11    129         3.0             3.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_move_iar(df, 'price_diff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 2",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-b58200bb8060>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcalc_move_iar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-51-45cc33ebb94a>\u001b[0m in \u001b[0;36mcalc_move_iar\u001b[0;34m(df, iar_col, iar_suffix, copy)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtmp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miar_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtmp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miar_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "calc_move_iar(df, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>price_diff</th>\n",
       "      <th>price_iar</th>\n",
       "      <th>price_diff_iar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>127</td>\n",
       "      <td>NaN</td>\n",
       "      <td>127.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128</td>\n",
       "      <td>1.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>131</td>\n",
       "      <td>3.0</td>\n",
       "      <td>386.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>1.0</td>\n",
       "      <td>518.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>133</td>\n",
       "      <td>1.0</td>\n",
       "      <td>651.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>132</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>783.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>130</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>913.0</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1043.0</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>128</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1171.0</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>127</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1298.0</td>\n",
       "      <td>-6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>126</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1424.0</td>\n",
       "      <td>-7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>129</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1553.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    price  price_diff  price_iar  price_diff_iar\n",
       "0     127         NaN      127.0             NaN\n",
       "1     128         1.0      255.0             1.0\n",
       "2     131         3.0      386.0             4.0\n",
       "3     132         1.0      518.0             5.0\n",
       "4     133         1.0      651.0             6.0\n",
       "5     132        -1.0      783.0            -1.0\n",
       "6     130        -2.0      913.0            -3.0\n",
       "7     130         0.0     1043.0            -3.0\n",
       "8     128        -2.0     1171.0            -5.0\n",
       "9     127        -1.0     1298.0            -6.0\n",
       "10    126        -1.0     1424.0            -7.0\n",
       "11    129         3.0     1553.0             3.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_move_iar(df, ['price','price_diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Columns must be same length as key",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-8c7502573174>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price_iar'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'price_diff_iar'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_move_iar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'price_diff'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3158\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3159\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3160\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3161\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3162\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_setitem_array\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3187\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3188\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3189\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Columns must be same length as key\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3190\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3191\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Columns must be same length as key"
     ]
    }
   ],
   "source": [
    "df[['price_iar', 'price_diff_iar']] = calc_move_iar(df, ['price','price_diff'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "iar_col='price_diff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>price_diff</th>\n",
       "      <th>move_iar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>127</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>131</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>133</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>132</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>130</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>128</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>127</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>126</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>129</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    price  price_diff  move_iar\n",
       "0     127         NaN       NaN\n",
       "1     128         1.0       1.0\n",
       "2     131         3.0       4.0\n",
       "3     132         1.0       5.0\n",
       "4     133         1.0       6.0\n",
       "5     132        -1.0      -1.0\n",
       "6     130        -2.0      -3.0\n",
       "7     130         0.0      -3.0\n",
       "8     128        -2.0      -5.0\n",
       "9     127        -1.0      -6.0\n",
       "10    126        -1.0      -7.0\n",
       "11    129         3.0       3.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp1 = df[iar_col].transform(lambda x: x.cumsum().sub(x.cumsum().mask(x >= 0).ffill(), fill_value=0), axis=0).replace(0, np.nan)\n",
    "tmp2 = df[iar_col].transform(lambda x: x.cumsum().sub(x.cumsum().mask(x <= 0).ffill(), fill_value=0), axis=0).replace(0, np.nan)\n",
    "df['move_iar'] = tmp1.fillna(tmp2).ffill()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     NaN\n",
       "1     NaN\n",
       "2     NaN\n",
       "3     NaN\n",
       "4     NaN\n",
       "5    -1.0\n",
       "6    -3.0\n",
       "7    -3.0\n",
       "8    -2.0\n",
       "9    -3.0\n",
       "10   -4.0\n",
       "11    3.0\n",
       "Name: price_diff, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[iar_col] = tmp1.fillna(tmp2).ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are a lot of missing targets. What do we do with them?\n",
    "- This becomes a semi-supervised learning problem since there is likely predictive information where there is no numerai target <br>\n",
    "- To fill them in, I'm going to take an educated guess and say that Numerai's targets are created based on profitable up moves in the market. <br>\n",
    "- The target they created is likely the following multi-class groups: **strong-short**, **short**, **no-trade**, **buy**, **strong-buy** - Let's find out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5337, 0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers_with_target = df_yahoo.loc[df_yahoo['target'].notnull(), 'bloomberg_ticker'].unique().tolist()\n",
    "tickers_without_target = df_yahoo.loc[df_yahoo['target'].isnull(), 'bloomberg_ticker'].unique().tolist()\n",
    "len(tickers_with_target), len(tickers_without_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UOFVjPU0yu7A"
   },
   "outputs": [],
   "source": [
    "ticker_groups = full_data.groupby('ticker')\n",
    "\n",
    "#create lagged features, lag 0 is that day's value, lag 1 is yesterday's value, etc\n",
    "num_days = 5\n",
    "for day in range(num_days+1):\n",
    "    full_data[f'RSI_quintile_lag_{day}'] = ticker_groups['RSI_quintile'].transform(lambda group: group.shift(day))\n",
    "full_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Es0O8RKKyu-a"
   },
   "outputs": [],
   "source": [
    "# create difference of the lagged features (change in RSI quintile by day)\n",
    "for day in range(num_days):\n",
    "    full_data[f'RSI_diff_{day}'] = full_data[f'RSI_quintile_lag_{day}'] - full_data[f'RSI_quintile_lag_{day + 1}']\n",
    "    full_data[f'RSI_abs_diff_{day}'] = np.abs(full_data[f'RSI_quintile_lag_{day}'] - full_data[f'RSI_quintile_lag_{day + 1}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xkrsTeFuyvBq"
   },
   "outputs": [],
   "source": [
    "feature_names = [f'RSI_quintile_lag_{num}' for num in range(num_days)] + [f'RSI_diff_{num}' for num in range(num_days)] + [f'RSI_abs_diff_{num}' for num in range(num_days)]\n",
    "print(f'Features for training:\\n {feature_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KahFrrERyvE5"
   },
   "outputs": [],
   "source": [
    "TARGET_NAME = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8dqoFqMzyvIG"
   },
   "outputs": [],
   "source": [
    "# read in Signals targets\n",
    "numerai_targets = 'https://numerai-signals-public-data.s3-us-west-2.amazonaws.com/signals_train_val.csv'\n",
    "targets = pd.read_csv(numerai_targets)\n",
    "targets['date'] = pd.to_datetime(targets['friday_date'], format='%Y%m%d')\n",
    "targets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0XWeLvZyvLr"
   },
   "outputs": [],
   "source": [
    "# the number of tickers per era has generally increased\n",
    "targets.groupby('date').apply(lambda x: len(x)).plot(kind='line', figsize=(10,4), title='Number of tickers per era')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wqC4bJX2yvO8"
   },
   "outputs": [],
   "source": [
    "# the target classes are imbalanced, but we can treat this like a regression problem\n",
    "targets.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVF1kpUkyvSe"
   },
   "outputs": [],
   "source": [
    "# the imbalance is consistent across eras with a constant class ratio of: 5%, 20%, 50%, 20%, 5%\n",
    "pivot_target = targets.groupby(['date','target']).apply(lambda x: len(x)).reset_index(1).pivot(columns='target',values=0)\n",
    "pivot_target.iloc[::20].plot(kind='bar', stacked=True, figsize=(9,3), title='Number of tickers in each class per era')\n",
    "\n",
    "stacked_data = pivot_target.apply(lambda x: x/sum(x), axis=1)\n",
    "stacked_data.iloc[::20].plot(kind='bar', stacked=True, figsize=(9,3), title='Proportion of tickers in each class per era')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7nFiUa-yvXy"
   },
   "outputs": [],
   "source": [
    "targets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dL1psmzPyvbs"
   },
   "outputs": [],
   "source": [
    "# merge our feature data with Numerai targets\n",
    "ML_data = pd.merge(full_data.reset_index(), targets, on=['date','ticker']).set_index('date')\n",
    "# print(f'Number of eras in data: {len(ML_data.index.unique())}')\n",
    "\n",
    "# for training and testing we want clean, complete data only\n",
    "ML_data.dropna(inplace=True)\n",
    "ML_data = ML_data[ML_data.index.weekday==4] # ensure we have only fridays\n",
    "ML_data = ML_data[ML_data.index.value_counts() > 200] # drop eras with under 200 observations per era"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UeymnV_nzT5d"
   },
   "outputs": [],
   "source": [
    "print(f'Number of eras in data: {len(ML_data.index.unique())}')\n",
    "ML_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zcLutuwHzT8_"
   },
   "outputs": [],
   "source": [
    "train_data = ML_data[ML_data['data_type'] == 'train']\n",
    "test_data = ML_data[ML_data['data_type'] == 'validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWE4J6ihzUAR"
   },
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor()\n",
    "model.fit(train_data[feature_names], train_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JmssRtBKzUDS"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,3))\n",
    "plt.bar(feature_names, model.feature_importances_)\n",
    "plt.xticks(rotation=70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "snk6cOZTzUHN"
   },
   "outputs": [],
   "source": [
    "PREDICTION_NAME = 'prediction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iwdod193zUKP"
   },
   "outputs": [],
   "source": [
    "train_data[PREDICTION_NAME] = model.predict(train_data[feature_names])\n",
    "test_data[PREDICTION_NAME] = model.predict(test_data[feature_names])\n",
    "\n",
    "#show prediction distribution, most should around the center\n",
    "test_data[PREDICTION_NAME].hist(bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5OK3cJvzUNw"
   },
   "outputs": [],
   "source": [
    "def score(df):\n",
    "    '''Takes df and calculates spearm correlation from pre-defined cols'''\n",
    "    # method=\"first\" breaks ties based on order in array\n",
    "    return np.corrcoef(\n",
    "        df[TARGET_NAME],\n",
    "        df[PREDICTION_NAME].rank(pct=True, method=\"first\")\n",
    "    )[0,1]\n",
    "\n",
    "def run_analytics(era_scores):\n",
    "    print(f\"Mean Correlation: {era_scores.mean():.4f}\")\n",
    "    print(f\"Median Correlation: {era_scores.median():.4f}\")\n",
    "    print(f\"Standard Deviation: {era_scores.std():.4f}\")\n",
    "    print('\\n')\n",
    "    print(f\"Mean Pseudo-Sharpe: {era_scores.mean()/era_scores.std():.4f}\")\n",
    "    print(f\"Median Pseudo-Sharpe: {era_scores.median()/era_scores.std():.4f}\")\n",
    "    print('\\n')\n",
    "    print(f'Hit Rate (% positive eras): {era_scores.apply(lambda x: np.sign(x)).value_counts()[1]/len(era_scores):.2%}')\n",
    "\n",
    "    era_scores.rolling(10).mean().plot(kind='line', title='Rolling Per Era Correlation Mean', figsize=(15,4))\n",
    "    plt.axhline(y=0.0, color=\"r\", linestyle=\"--\"); plt.show()\n",
    "\n",
    "    era_scores.cumsum().plot(title='Cumulative Sum of Era Scores', figsize=(15,4))\n",
    "    plt.axhline(y=0.0, color=\"r\", linestyle=\"--\"); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_pLA3_PzURS"
   },
   "outputs": [],
   "source": [
    "# spearman scores by era\n",
    "train_era_scores = train_data.groupby(train_data.index).apply(score)\n",
    "test_era_scores = test_data.groupby(test_data.index).apply(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JHPMlABQzUUd"
   },
   "outputs": [],
   "source": [
    "#train scores, in-sample and will be significantly overfit\n",
    "run_analytics(train_era_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFsUxckKzUYj"
   },
   "outputs": [],
   "source": [
    "#test scores, out of sample\n",
    "run_analytics(test_era_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t30vuingzUbd"
   },
   "outputs": [],
   "source": [
    "# choose data as of most recent friday\n",
    "last_friday = datetime.now() + relativedelta(weekday=FR(-1))\n",
    "date_string = last_friday.strftime('%Y-%m-%d')\n",
    "\n",
    "live_data = full_data.loc[date_string].copy()\n",
    "live_data.dropna(subset=feature_names, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WEhtlB4DzUfL"
   },
   "outputs": [],
   "source": [
    "print(f\"Number of live tickers to submit: {len(live_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jPmPsAGYztjQ"
   },
   "outputs": [],
   "source": [
    "live_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2er-yacztmB"
   },
   "outputs": [],
   "source": [
    "live_data[PREDICTION_NAME] = model.predict(live_data[feature_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISCnCGQDztop"
   },
   "outputs": [],
   "source": [
    "diagnostic_df = pd.concat([test_data, live_data])\n",
    "diagnostic_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R5pfv7VTztr9"
   },
   "outputs": [],
   "source": [
    "diagnostic_df['friday_date'] = diagnostic_df.friday_date.fillna(last_friday.strftime('%Y%m%d')).astype(int)\n",
    "diagnostic_df['data_type'] = diagnostic_df.data_type.fillna('live')\n",
    "diagnostic_df[['ticker','friday_date','data_type','prediction']].reset_index(drop=True).to_csv('example_signal_upload.csv', index=False)\n",
    "diagnostic_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGWNHsMDztuq"
   },
   "outputs": [],
   "source": [
    "# format predictions to match Numerai submission format\n",
    "predictions = live_data[['ticker', PREDICTION_NAME]].copy()\n",
    "\n",
    "# choose account\n",
    "ACCOUNT_NAME = 'ENTER_ACCOUNT_NAME'\n",
    "\n",
    "# write predictions to csv\n",
    "live_data[['ticker', PREDICTION_NAME]].to_csv(f\"{ACCOUNT_NAME} {datetime.now().strftime('%Y%m%d')}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBoZIk_xztxf"
   },
   "outputs": [],
   "source": [
    "def submit_model(account_name):\n",
    "    filename = f\"{account_name} {datetime.now().strftime('%Y%m%d')}.csv\"\n",
    "    model_id = napi.get_models()[f'{account_name}']\n",
    "    submission = napi.upload_predictions(filename, model_id=model_id)\n",
    "    print(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7M_Y8bYzt07"
   },
   "outputs": [],
   "source": [
    "submit_model(ACCOUNT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pd23tVq_zt3_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "80DA0_H-zt7V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      df_yahoo:  4.4 GiB\n",
      "                       targets: 623.9 MiB\n",
      "                    ticker_map:  1.0 MiB\n",
      "              eligible_tickers: 338.2 KiB\n",
      "                 valid_tickers: 47.3 KiB\n",
      "                       TICKERS: 42.0 KiB\n",
      "                           _40:  9.1 KiB\n",
      "                           _34:  8.7 KiB\n",
      "                           _30:  7.8 KiB\n",
      "                           _26:  7.7 KiB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
    "                         key= lambda x: -x[1])[:10]:\n",
    "    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ep_UuQiVzt-X"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yn45a8_ozuBK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_fo8lf2BzuEe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def download_yfinance_data(tickers,\n",
      "                           intervals_to_download=['1d', '1h'],\n",
      "                           num_workers=1,\n",
      "                           join_method='outer',\n",
      "                           max_intraday_lookback_days=363,\n",
      "                           **yfinance_params):\n",
      "    \"\"\"\n",
      "    Parameters\n",
      "    __________\n",
      "\n",
      "    See yfinance.download docs for a detailed description of yfinance parameters\n",
      "\n",
      "    tickers : string separated by space tickers to pass to yfinance.download (e.g. \"AAPL MSFT FB\")\n",
      "    intervals_to_download : list of intervals to download OHLCV data for each stock (e.g. ['1w', '1d', '1h'])\n",
      "    num_workers : number of threads used to download the data\n",
      "        so far only 1 thread is implemented\n",
      "    join_method : can be 'inner', 'left', 'right' or 'outer'\n",
      "        if 'outer' then all dates will be present\n",
      "        if 'left' then all dates from the left most table will be present\n",
      "        if 'right' then all dates from the left most table will be present\n",
      "        if 'inner' then all dates must match for each ticker\n",
      "    **yfinance_params : dict - passed to yfinance.dowload(yfinance_params)\n",
      "\n",
      "    NOTE: passing some intervals return unreliable stock data (e.g. '3mo' returns many NA data points when they should not be NA)\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    yfinance_params2 = yfinance_params.copy() # create a copy for min / hour pulls because the start date can only go back 60 days\n",
      "\n",
      "    if num_workers == 1:\n",
      "\n",
      "        list_of_dfs = []\n",
      "\n",
      "        for i in intervals_to_download:\n",
      "\n",
      "            yfinance_params['interval'] = i\n",
      "\n",
      "            if i.endswith('m') or i.endswith('h'): # min or hr\n",
      "\n",
      "                yfinance_params2['interval'] = i\n",
      "                yfinance_params2['start'] = str(datetime.datetime.today().date() - datetime.timedelta(days=max_intraday_lookback_days))\n",
      "\n",
      "\n",
      "                df_i = yfinance.download(tickers, **yfinance_params2).\\\n",
      "                        stack().\\\n",
      "                        add_suffix('_' + str(i)).\\\n",
      "                        reset_index(level=1).\\\n",
      "                        rename(columns={'level_1' : 'ticker'})\n",
      "\n",
      "                df_i = df_i.pivot_table(index=df_i.index.date, columns = ['ticker', df_i.index.hour]).stack(level=1)\n",
      "                df_i.columns = list(pd.Index([str(e[0]).lower() + '_' + str(e[1]).lower() for e in df_i.columns.tolist()]).str.replace(' ', '_'))\n",
      "\n",
      "            else:\n",
      "                df_i = yfinance.download(tickers, **yfinance_params).\\\n",
      "                        stack().\\\n",
      "                        add_suffix('_' + str(i))\n",
      "\n",
      "                df_i.columns = [col.replace(' ', '_').lower() for col in df_i.columns]\n",
      "\n",
      "            df_i.index.names = ['date', 'ticker']\n",
      "\n",
      "            list_of_dfs.append(df_i)\n",
      "\n",
      "\n",
      "        df_yahoo = reduce(lambda x, y: pd.merge(x, y, how=join_method, left_index=True, right_index=True), list_of_dfs)\n",
      "#         df_yahoo.reset_index(level=1, inplace=True)\n",
      "\n",
      "    else:\n",
      "        return 'multi-threading not implemented yet. Set num_workers to 1.'\n",
      "\n",
      "    return df_yahoo\n"
     ]
    }
   ],
   "source": [
    "import inspect as i\n",
    "import sys\n",
    "sys.stdout.write(i.getsource(download_yfinance_data))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "build_numerai_dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
